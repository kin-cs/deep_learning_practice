{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://github.com/devnag/pytorch-generative-adversarial-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Data [Data and Variances]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# data params\n",
    "\n",
    "data_mean = 4\n",
    "data_sd = 1.25\n",
    "\n",
    "# Model params\n",
    "g_input_size = 1 # input a random noise\n",
    "g_hidden_size = 100\n",
    "g_output_size = 1\n",
    "\n",
    "d_input_size = 100 # input minibatch\n",
    "d_hidden_size = 50\n",
    "d_output_size = 1 # Real or Fake\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "d_learning_rate =2e-4\n",
    "g_learning_rate =2e-4\n",
    "optim_beta = (0.9, 0.999)\n",
    "num_epochs = 30000\n",
    "print_interval = 200\n",
    "d_steps = 1  # training rate on d\n",
    "g_steps = 1\n",
    "\n",
    "# ### Uncomment only one of these\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "\n",
    "(name, preprocess, d_input_func) = (\"Data and Variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x*2)\n",
    "\n",
    "print('Using Data [%s]' % (name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Data: target data + generator's random input data\n",
    "\n",
    "def get_distribution_sampler(mu, stigma):  # Normal Distribution Target Data\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, stigma, (1, n)))\n",
    "\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)\n",
    "\n",
    "###### Models: g model and d model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        x = self.map3(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        x = F.sigmoid(self.map3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]\n",
    "\n",
    "def decorate_with_diffs(data, exponent):\n",
    "    mean = torch.mean(data.data, 1)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    return torch.cat([data, diffs], 1)\n",
    "\n",
    "d_sampler = get_distribution_sampler(data_mean, data_sd)\n",
    "gi_sampler = get_generator_input_sampler()\n",
    "\n",
    "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_beta)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=d_learning_rate, betas=optim_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "\tD: 0.7825380563735962/0.7065199017524719 \n",
      "\tG: 0.6832863092422485 (Real: [3.9759718489646914, 1.1967442681418134], Fake: [-0.14098717421293258, 0.0051190178644274871]\n",
      "\n",
      ") \n",
      "200: \n",
      "\tD: 8.523499673174229e-06/0.4610758125782013 \n",
      "\tG: 1.0527156591415405 (Real: [3.769114253818989, 1.4556654847814041], Fake: [-0.073771458789706229, 0.10138906873339411]\n",
      "\n",
      ") \n",
      "400: \n",
      "\tD: 0.059471286833286285/0.6597369909286499 \n",
      "\tG: 0.8466678857803345 (Real: [3.8474287581443787, 1.2279383694054642], Fake: [0.74464927256107327, 0.54593853368662348]\n",
      "\n",
      ") \n",
      "600: \n",
      "\tD: 1.3463455438613892/0.5777817964553833 \n",
      "\tG: 0.9558966755867004 (Real: [3.8410544872283934, 1.3291238175658113], Fake: [3.995078594684601, 1.1316642876261958]\n",
      "\n",
      ") \n",
      "800: \n",
      "\tD: 1.3029022216796875/0.5864706039428711 \n",
      "\tG: 0.6268869042396545 (Real: [3.837817888855934, 1.2236604420192185], Fake: [6.3595729708671573, 0.99251129699395868]\n",
      "\n",
      ") \n",
      "1000: \n",
      "\tD: 0.7391831278800964/0.44163423776626587 \n",
      "\tG: 1.297122597694397 (Real: [3.9005211305618288, 1.2433991680558574], Fake: [6.0744481039047242, 1.1764154255041324]\n",
      "\n",
      ") \n",
      "1200: \n",
      "\tD: 0.8048421144485474/0.6607333421707153 \n",
      "\tG: 0.716468870639801 (Real: [3.8354432819783688, 1.268785590707403], Fake: [3.9953903663158417, 1.1814488264669847]\n",
      "\n",
      ") \n",
      "1400: \n",
      "\tD: 0.7111026644706726/0.8250300884246826 \n",
      "\tG: 0.5628978610038757 (Real: [3.9534019887447358, 1.1603111231715062], Fake: [2.9254365515708924, 1.1172338815510559]\n",
      "\n",
      ") \n",
      "1600: \n",
      "\tD: 1.0197315216064453/0.555260181427002 \n",
      "\tG: 0.7460969686508179 (Real: [3.8263078269362452, 1.4629935689033478], Fake: [3.5216369616985319, 1.3209083360359768]\n",
      "\n",
      ") \n",
      "1800: \n",
      "\tD: 0.9889599680900574/0.5567268133163452 \n",
      "\tG: 0.7477815747261047 (Real: [3.7767012214660642, 1.3373143832300503], Fake: [4.6720193970203399, 1.4166031639949366]\n",
      "\n",
      ") \n",
      "2000: \n",
      "\tD: 0.7894765734672546/0.7113645672798157 \n",
      "\tG: 0.8956349492073059 (Real: [3.9779997247457506, 1.3354749051172321], Fake: [4.6154104506969453, 1.2180332578417943]\n",
      "\n",
      ") \n",
      "2200: \n",
      "\tD: 0.5145658254623413/0.7654956579208374 \n",
      "\tG: 0.5539729595184326 (Real: [3.9494486677646639, 1.1839439878466893], Fake: [3.5076207125186922, 1.1603358255292255]\n",
      "\n",
      ") \n",
      "2400: \n",
      "\tD: 0.7226130962371826/0.6448507905006409 \n",
      "\tG: 0.6079666018486023 (Real: [4.0496544075012206, 1.031507994383541], Fake: [3.2799845743179321, 1.273135011165444]\n",
      "\n",
      ") \n",
      "2600: \n",
      "\tD: 0.6528445482254028/0.6125081181526184 \n",
      "\tG: 0.9668415188789368 (Real: [4.2221565133333208, 1.3409113307528586], Fake: [4.7608408439159398, 1.4363654461464557]\n",
      "\n",
      ") \n",
      "2800: \n",
      "\tD: 0.749576985836029/0.6244361996650696 \n",
      "\tG: 0.6065406203269958 (Real: [3.9629299843311312, 1.3291296477649543], Fake: [3.9406230866909029, 1.3278498052438137]\n",
      "\n",
      ") \n",
      "3000: \n",
      "\tD: 0.6331180334091187/0.7967450022697449 \n",
      "\tG: 0.6715297698974609 (Real: [3.8851709568500521, 1.2421355019101274], Fake: [3.7510100138187408, 1.0004056675743178]\n",
      "\n",
      ") \n",
      "3200: \n",
      "\tD: 0.6047332286834717/0.6537437438964844 \n",
      "\tG: 0.7252002954483032 (Real: [4.089575456976891, 1.1843168387665244], Fake: [4.5809381377696994, 1.2512072269636541]\n",
      "\n",
      ") \n",
      "3400: \n",
      "\tD: 0.6993434429168701/0.6927697062492371 \n",
      "\tG: 0.8474262356758118 (Real: [4.044215769767761, 1.2179838123536793], Fake: [3.9316346049308777, 1.2920368721048772]\n",
      "\n",
      ") \n",
      "3600: \n",
      "\tD: 0.5991461277008057/0.7246587872505188 \n",
      "\tG: 0.5331097841262817 (Real: [3.9645986974239351, 1.158012726217547], Fake: [3.835928587913513, 0.93601158345441071]\n",
      "\n",
      ") \n",
      "3800: \n",
      "\tD: 0.8383700251579285/0.641550600528717 \n",
      "\tG: 0.7196674942970276 (Real: [3.9365694290399551, 1.2942823036051772], Fake: [4.9561099457740783, 0.97761721318180939]\n",
      "\n",
      ") \n",
      "4000: \n",
      "\tD: 0.717606246471405/0.6396855711936951 \n",
      "\tG: 0.6720210313796997 (Real: [3.9456494736671446, 1.2027278583579524], Fake: [3.7779517459869383, 1.1035873350829164]\n",
      "\n",
      ") \n",
      "4200: \n",
      "\tD: 0.7422552704811096/0.7242023348808289 \n",
      "\tG: 0.8037849068641663 (Real: [4.1833157491683961, 1.3446385496307822], Fake: [4.5206895363330837, 1.3596066258731669]\n",
      "\n",
      ") \n",
      "4400: \n",
      "\tD: 0.5945946574211121/0.6020016074180603 \n",
      "\tG: 0.7895888090133667 (Real: [4.0449817192554471, 1.1914287220628728], Fake: [3.4513450932502745, 1.4415428570201529]\n",
      "\n",
      ") \n",
      "4600: \n",
      "\tD: 0.5176973342895508/0.6657666563987732 \n",
      "\tG: 0.7433793544769287 (Real: [4.0505584114789963, 1.0621904963512832], Fake: [3.4342786842584609, 1.2687108126456392]\n",
      "\n",
      ") \n",
      "4800: \n",
      "\tD: 0.8753346800804138/0.5174176096916199 \n",
      "\tG: 0.9380208849906921 (Real: [4.1347071647644045, 1.2400124568805959], Fake: [4.7461282134056093, 1.0733514880719952]\n",
      "\n",
      ") \n",
      "5000: \n",
      "\tD: 0.6494466066360474/0.6322879791259766 \n",
      "\tG: 0.7384132742881775 (Real: [3.9899263763427735, 1.3288270598142442], Fake: [3.3689882445335386, 1.2054292177251713]\n",
      "\n",
      ") \n",
      "5200: \n",
      "\tD: 0.7507975101470947/0.6325315237045288 \n",
      "\tG: 0.6065201759338379 (Real: [3.6906242910027505, 1.1977331028970395], Fake: [4.5182718336582184, 1.3258403896672488]\n",
      "\n",
      ") \n",
      "5400: \n",
      "\tD: 0.7410237193107605/0.6605173349380493 \n",
      "\tG: 0.44342729449272156 (Real: [4.1757186233997343, 1.2588979106623883], Fake: [3.7163417315483094, 1.1537098372113967]\n",
      "\n",
      ") \n",
      "5600: \n",
      "\tD: 0.7425218224525452/0.6360085606575012 \n",
      "\tG: 0.5870519876480103 (Real: [3.9951751667261122, 1.1588463932349085], Fake: [3.9645376336574554, 1.1607519348797302]\n",
      "\n",
      ") \n",
      "5800: \n",
      "\tD: 0.8360890746116638/0.6330447793006897 \n",
      "\tG: 0.6308926343917847 (Real: [4.0508445943519469, 1.1585042830687748], Fake: [4.3219243407249452, 1.1557811535527076]\n",
      "\n",
      ") \n",
      "6000: \n",
      "\tD: 0.7998639941215515/0.5521352291107178 \n",
      "\tG: 0.4886581003665924 (Real: [3.8612984699010848, 1.1770486620292386], Fake: [3.6830725681781771, 1.2165474472429572]\n",
      "\n",
      ") \n",
      "6200: \n",
      "\tD: 0.7469801306724548/0.5983738899230957 \n",
      "\tG: 0.7234181761741638 (Real: [3.8493455195426942, 1.3307739770102158], Fake: [4.1251903522014617, 1.3315884253066776]\n",
      "\n",
      ") \n",
      "6400: \n",
      "\tD: 0.6041321754455566/0.3833812475204468 \n",
      "\tG: 0.8113942742347717 (Real: [4.2087968784570693, 1.1729403166943138], Fake: [3.3425970834493639, 1.5183881509827475]\n",
      "\n",
      ") \n",
      "6600: \n",
      "\tD: 0.5558097958564758/0.7777687311172485 \n",
      "\tG: 0.607410192489624 (Real: [3.800124580860138, 1.3084301977737596], Fake: [3.9869490945339203, 1.3388867753666907]\n",
      "\n",
      ") \n",
      "6800: \n",
      "\tD: 0.7417035698890686/0.7543466687202454 \n",
      "\tG: 0.777125358581543 (Real: [4.0046659988164901, 1.1401591287252413], Fake: [4.3919760286808014, 1.1180106131031373]\n",
      "\n",
      ") \n",
      "7000: \n",
      "\tD: 0.5733988881111145/0.6784798502922058 \n",
      "\tG: 0.7472833395004272 (Real: [3.7647612947225571, 1.2544329511109684], Fake: [3.6430989333987238, 1.0412129930804801]\n",
      "\n",
      ") \n",
      "7200: \n",
      "\tD: 0.9006748795509338/0.6054933667182922 \n",
      "\tG: 0.7744939923286438 (Real: [4.0378747820854191, 1.1993737906214383], Fake: [4.6364300572872166, 1.4723504852146456]\n",
      "\n",
      ") \n",
      "7400: \n",
      "\tD: 0.686988890171051/0.7725765705108643 \n",
      "\tG: 0.6975595951080322 (Real: [3.8808340430259705, 1.1587410966122826], Fake: [3.3652955073118211, 1.0910207091370123]\n",
      "\n",
      ") \n",
      "7600: \n",
      "\tD: 0.7440630197525024/0.6465417146682739 \n",
      "\tG: 0.907966673374176 (Real: [3.8727958023548128, 1.1718323994064184], Fake: [4.2921868598461153, 1.2812138048886506]\n",
      "\n",
      ") \n",
      "7800: \n",
      "\tD: 0.8720616102218628/0.7724989056587219 \n",
      "\tG: 0.5541694164276123 (Real: [3.8132005965709688, 1.3264969306336716], Fake: [4.0022089219093324, 1.401415732994896]\n",
      "\n",
      ") \n",
      "8000: \n",
      "\tD: 0.3221183717250824/0.4570951461791992 \n",
      "\tG: 0.6793397068977356 (Real: [4.0224151361733673, 1.3533035967980829], Fake: [3.8240581244230269, 1.0016708841223285]\n",
      "\n",
      ") \n",
      "8200: \n",
      "\tD: 0.47211605310440063/0.3236779570579529 \n",
      "\tG: 0.7428644895553589 (Real: [3.9945829302072524, 1.2988184636378297], Fake: [4.4348925769329073, 1.2095619665575039]\n",
      "\n",
      ") \n",
      "8400: \n",
      "\tD: 0.6201061606407166/0.725579559803009 \n",
      "\tG: 0.6458191871643066 (Real: [3.9920197939872741, 1.3343400418236824], Fake: [4.1758509993553163, 1.2347955776208415]\n",
      "\n",
      ") \n",
      "8600: \n",
      "\tD: 0.6020156145095825/0.5316013693809509 \n",
      "\tG: 0.6839220523834229 (Real: [3.9551265686750412, 1.2283751487784338], Fake: [3.4188391542434693, 1.3976487754339872]\n",
      "\n",
      ") \n",
      "8800: \n",
      "\tD: 0.8441157937049866/0.4758870005607605 \n",
      "\tG: 0.6856948733329773 (Real: [3.8685652208328247, 1.2310733102694289], Fake: [4.7477639281749724, 1.0873185636027605]\n",
      "\n",
      ") \n",
      "9000: \n",
      "\tD: 0.6794867515563965/0.6965354681015015 \n",
      "\tG: 0.741992175579071 (Real: [3.9606399261951446, 1.1567175154227751], Fake: [3.7946293440461161, 1.1068850106378985]\n",
      "\n",
      ") \n",
      "9200: \n",
      "\tD: 0.6867879629135132/0.5781430602073669 \n",
      "\tG: 0.7597227096557617 (Real: [4.1781337982416149, 1.30816447385884], Fake: [3.7540445309877395, 1.1909940780638786]\n",
      "\n",
      ") \n",
      "9400: \n",
      "\tD: 1.109010934829712/0.4439573287963867 \n",
      "\tG: 1.337562918663025 (Real: [4.0713827633857731, 1.2565307174653229], Fake: [4.5118996030092235, 1.4039802745288157]\n",
      "\n",
      ") \n",
      "9600: \n",
      "\tD: 0.517031192779541/0.30758681893348694 \n",
      "\tG: 0.6587429046630859 (Real: [3.8465171480178832, 1.2552569264708708], Fake: [3.425035597085953, 1.3904338552853235]\n",
      "\n",
      ") \n",
      "9800: \n",
      "\tD: 0.713236927986145/0.726350724697113 \n",
      "\tG: 0.8749570846557617 (Real: [4.0373934721946716, 1.2018371377607713], Fake: [4.2945045578479766, 1.0591962323627517]\n",
      "\n",
      ") \n",
      "10000: \n",
      "\tD: 0.553780198097229/0.2144443392753601 \n",
      "\tG: 0.5383395552635193 (Real: [3.8016598224639893, 1.2338055526930989], Fake: [4.0695791602134701, 1.4729401031180329]\n",
      "\n",
      ") \n",
      "10200: \n",
      "\tD: 0.5867589116096497/0.6408085823059082 \n",
      "\tG: 0.7633023262023926 (Real: [3.8043736332654952, 1.3437566123361782], Fake: [3.673956563472748, 1.0919434987024041]\n",
      "\n",
      ") \n",
      "10400: \n",
      "\tD: 0.7278483510017395/0.6711881160736084 \n",
      "\tG: 0.9677021503448486 (Real: [3.8802877473831177, 1.2125917603026011], Fake: [4.2359970521926877, 1.3108653611078767]\n",
      "\n",
      ") \n",
      "10600: \n",
      "\tD: 0.8380789160728455/0.5287536978721619 \n",
      "\tG: 0.5179904103279114 (Real: [4.2217915952205658, 1.1227338742225679], Fake: [4.0686049205064769, 1.2836326849113018]\n",
      "\n",
      ") \n",
      "10800: \n",
      "\tD: 0.6308600306510925/0.5327215790748596 \n",
      "\tG: 0.6956289410591125 (Real: [3.9123110300302506, 1.3624786098095523], Fake: [3.7994416123628616, 1.184415774901622]\n",
      "\n",
      ") \n",
      "11000: \n",
      "\tD: 0.6473876237869263/0.40833649039268494 \n",
      "\tG: 0.6934544444084167 (Real: [4.26543790102005, 1.2622882268084838], Fake: [4.5075603151321415, 1.2097557605624885]\n",
      "\n",
      ") \n",
      "11200: \n",
      "\tD: 1.5656991004943848/0.4478089511394501 \n",
      "\tG: 0.9653059840202332 (Real: [4.0472436916828158, 1.298510920910829], Fake: [3.9136655294895171, 1.1692155620345712]\n",
      "\n",
      ") \n",
      "11400: \n",
      "\tD: 0.6621420383453369/0.6506896615028381 \n",
      "\tG: 0.8493806719779968 (Real: [4.1081800484657292, 1.0992984354940742], Fake: [3.5941523826122284, 1.1228791887922986]\n",
      "\n",
      ") \n",
      "11600: \n",
      "\tD: 0.44105470180511475/0.8624141812324524 \n",
      "\tG: 0.5811513066291809 (Real: [4.2406063523888591, 1.2027457887542936], Fake: [4.2101347064971923, 1.3566784103658724]\n",
      "\n",
      ") \n",
      "11800: \n",
      "\tD: 0.18722499907016754/0.36241236329078674 \n",
      "\tG: 0.9705148339271545 (Real: [3.9458676427602768, 1.2068762420551111], Fake: [4.3991503679752348, 1.2021030040408724]\n",
      "\n",
      ") \n",
      "12000: \n",
      "\tD: 0.3226540982723236/0.9852142930030823 \n",
      "\tG: 1.1844130754470825 (Real: [3.8732195326685908, 1.4171546345261685], Fake: [3.8650562059879303, 1.3393463559661898]\n",
      "\n",
      ") \n",
      "12200: \n",
      "\tD: 0.2637050449848175/0.5447176694869995 \n",
      "\tG: 0.7461296319961548 (Real: [4.0956754353642459, 1.3093075794496645], Fake: [3.5525273531675339, 1.1611301622975758]\n",
      "\n",
      ") \n",
      "12400: \n",
      "\tD: 1.116164207458496/0.33378762006759644 \n",
      "\tG: 1.3063865900039673 (Real: [3.6955942130088806, 1.1056678002035512], Fake: [4.23707871556282, 1.3021733714165815]\n",
      "\n",
      ") \n",
      "12600: \n",
      "\tD: 0.8358883857727051/0.7087987661361694 \n",
      "\tG: 0.7275850176811218 (Real: [3.7942377477884293, 1.2576574936324878], Fake: [3.9488907372951507, 1.2878045059676901]\n",
      "\n",
      ") \n",
      "12800: \n",
      "\tD: 0.36840522289276123/0.4978714883327484 \n",
      "\tG: 0.8858471512794495 (Real: [3.939993715286255, 1.2916730972011206], Fake: [4.0538694888353346, 1.1772749857068407]\n",
      "\n",
      ") \n",
      "13000: \n",
      "\tD: 0.6753943562507629/0.5346294641494751 \n",
      "\tG: 0.7731945514678955 (Real: [3.9908615118265152, 1.2856446612162333], Fake: [4.1744468688964842, 1.1751967174791191]\n",
      "\n",
      ") \n",
      "13200: \n",
      "\tD: 1.0816034078598022/0.42025870084762573 \n",
      "\tG: 1.0880284309387207 (Real: [4.0480894708633421, 1.0646710720365593], Fake: [4.0059373521804806, 1.3344460000504079]\n",
      "\n",
      ") \n",
      "13400: \n",
      "\tD: 0.530941367149353/1.1251060962677002 \n",
      "\tG: 0.5303264856338501 (Real: [4.0808347827196121, 1.3799905716442387], Fake: [3.4212637960910799, 1.2953873077080378]\n",
      "\n",
      ") \n",
      "13600: \n",
      "\tD: 0.6233006715774536/0.2874557673931122 \n",
      "\tG: 1.2948715686798096 (Real: [3.9896428787708285, 1.2267153082612885], Fake: [4.1711519646644595, 1.1874729328270797]\n",
      "\n",
      ") \n",
      "13800: \n",
      "\tD: 0.1990424394607544/0.42468544840812683 \n",
      "\tG: 1.1344430446624756 (Real: [3.8791875535249711, 1.2403972043423741], Fake: [4.0863246035575864, 1.3351452949344125]\n",
      "\n",
      ") \n",
      "14000: \n",
      "\tD: 0.23156385123729706/0.5759100317955017 \n",
      "\tG: 0.8494551181793213 (Real: [3.9626850545406342, 1.2278891116121136], Fake: [4.0821893310546873, 1.1662140386736681]\n",
      "\n",
      ") \n",
      "14200: \n",
      "\tD: 1.135972261428833/0.46903330087661743 \n",
      "\tG: 1.1790897846221924 (Real: [3.962412122488022, 1.3622798518442814], Fake: [4.0741180229187011, 1.1297139352312009]\n",
      "\n",
      ") \n",
      "14400: \n",
      "\tD: 0.5454089641571045/0.8357199430465698 \n",
      "\tG: 1.0837899446487427 (Real: [4.0273183929920195, 1.2029060307879513], Fake: [3.9411736798286436, 1.2008558162712846]\n",
      "\n",
      ") \n",
      "14600: \n",
      "\tD: 0.4934779107570648/0.5237612128257751 \n",
      "\tG: 0.6683372259140015 (Real: [4.1769300651550294, 1.0731103057051254], Fake: [3.9240784525871275, 1.1481449838857554]\n",
      "\n",
      ") \n",
      "14800: \n",
      "\tD: 0.5184100270271301/0.4597344398498535 \n",
      "\tG: 0.7553651928901672 (Real: [4.2032246494293215, 0.98359859916610248], Fake: [3.9264760720729828, 1.2857401575980199]\n",
      "\n",
      ") \n",
      "15000: \n",
      "\tD: 0.47575899958610535/0.8144672513008118 \n",
      "\tG: 1.2794486284255981 (Real: [4.1419493186473844, 1.2796339847527971], Fake: [4.0858817720413212, 1.2553986460934397]\n",
      "\n",
      ") \n",
      "15200: \n",
      "\tD: 0.214747354388237/0.1397213339805603 \n",
      "\tG: 1.9528441429138184 (Real: [3.9822086966037751, 1.1829614049856907], Fake: [3.8572555887699127, 1.219000579649234]\n",
      "\n",
      ") \n",
      "15400: \n",
      "\tD: 0.23428581655025482/0.7893685102462769 \n",
      "\tG: 1.7325832843780518 (Real: [3.9399572646617891, 1.3070901317010248], Fake: [4.0080774605274199, 1.2886370271252838]\n",
      "\n",
      ") \n",
      "15600: \n",
      "\tD: 0.9829927682876587/0.5446873307228088 \n",
      "\tG: 2.2161266803741455 (Real: [3.9412548691034317, 1.2130438791477476], Fake: [3.9097407484054565, 1.4705949129208131]\n",
      "\n",
      ") \n",
      "15800: \n",
      "\tD: 0.1587064266204834/0.4048140048980713 \n",
      "\tG: 1.316571593284607 (Real: [4.1503004419803622, 1.3006835176125937], Fake: [4.1676123237609861, 1.1829231515101739]\n",
      "\n",
      ") \n",
      "16000: \n",
      "\tD: 0.31377461552619934/0.2834736406803131 \n",
      "\tG: 1.787764549255371 (Real: [3.9670131860673425, 1.3262331405987982], Fake: [3.7404857212305069, 1.393750736424535]\n",
      "\n",
      ") \n",
      "16200: \n",
      "\tD: 0.6468729376792908/0.8509253859519958 \n",
      "\tG: 0.7976799607276917 (Real: [4.0572063565254215, 1.3172680092446203], Fake: [4.037464253902435, 1.0988791472525814]\n",
      "\n",
      ") \n",
      "16400: \n",
      "\tD: 0.9059230089187622/0.533085286617279 \n",
      "\tG: 1.4977167844772339 (Real: [4.0839261496067047, 1.204546731737961], Fake: [3.8562199622392654, 1.3014310672953011]\n",
      "\n",
      ") \n",
      "16600: \n",
      "\tD: 0.22848284244537354/0.3019748032093048 \n",
      "\tG: 0.8708375692367554 (Real: [4.0138158470392229, 1.1954600506536497], Fake: [3.8938619160652159, 1.1542696737428668]\n",
      "\n",
      ") \n",
      "16800: \n",
      "\tD: 0.6738287806510925/0.7858604788780212 \n",
      "\tG: 1.3779598474502563 (Real: [4.1159602975845333, 1.1381860740452576], Fake: [4.2086083436012265, 1.0415948834982938]\n",
      "\n",
      ") \n",
      "17000: \n",
      "\tD: 0.4753386676311493/0.2855483889579773 \n",
      "\tG: 0.3529646396636963 (Real: [3.9604185020923612, 0.99822292223728348], Fake: [4.0590233206748962, 1.3792500680626851]\n",
      "\n",
      ") \n",
      "17200: \n",
      "\tD: 0.2352808713912964/0.21024896204471588 \n",
      "\tG: 0.5986896753311157 (Real: [3.9631617581844329, 1.2135364629186514], Fake: [4.2106534051895146, 1.0770176144646082]\n",
      "\n",
      ") \n",
      "17400: \n",
      "\tD: 0.3248034715652466/0.7686821818351746 \n",
      "\tG: 1.272875428199768 (Real: [3.9478980195522309, 1.2935809450041369], Fake: [3.2574841400980947, 1.3697793977988248]\n",
      "\n",
      ") \n",
      "17600: \n",
      "\tD: 0.31058692932128906/0.4619486331939697 \n",
      "\tG: 0.4330117106437683 (Real: [3.9623492312431337, 1.0926413448263641], Fake: [4.5700329375267028, 1.2019978451523636]\n",
      "\n",
      ") \n",
      "17800: \n",
      "\tD: 0.05040396377444267/0.4680619239807129 \n",
      "\tG: 1.3888510465621948 (Real: [4.1597754418849942, 1.3509490276770995], Fake: [4.2876769518852234, 1.2541530957238263]\n",
      "\n",
      ") \n",
      "18000: \n",
      "\tD: 0.8231638669967651/0.46294257044792175 \n",
      "\tG: 0.8741451501846313 (Real: [4.0201953434944153, 1.2552318037998194], Fake: [3.7025236991047858, 1.1430550068210057]\n",
      "\n",
      ") \n",
      "18200: \n",
      "\tD: 0.728144645690918/0.3897278606891632 \n",
      "\tG: 0.9443482756614685 (Real: [3.9662786436080935, 1.2542706903677285], Fake: [4.3426185905933377, 1.2619846633638461]\n",
      "\n",
      ") \n",
      "18400: \n",
      "\tD: 0.20619933307170868/1.011352777481079 \n",
      "\tG: 1.4663649797439575 (Real: [4.1302562916278838, 1.2008720152085119], Fake: [4.6907339441776275, 1.1633491807865175]\n",
      "\n",
      ") \n",
      "18600: \n",
      "\tD: 0.23963549733161926/0.38908690214157104 \n",
      "\tG: 0.3878259062767029 (Real: [4.1817540383338931, 1.205279173478828], Fake: [4.3467664968967439, 1.2338347185298348]\n",
      "\n",
      ") \n",
      "18800: \n",
      "\tD: 0.9206527471542358/0.6781947016716003 \n",
      "\tG: 1.8211160898208618 (Real: [3.9952517598867416, 1.2208354142164122], Fake: [3.0122763636894523, 1.1223315535976452]\n",
      "\n",
      ") \n",
      "19000: \n",
      "\tD: 1.336667776107788/0.30660173296928406 \n",
      "\tG: 1.451927661895752 (Real: [3.9046983468532561, 1.3908626560356945], Fake: [5.1714647591114042, 1.3507053516937046]\n",
      "\n",
      ") \n",
      "19200: \n",
      "\tD: 0.1440414935350418/1.5932446718215942 \n",
      "\tG: 0.8261272311210632 (Real: [3.4198241543769838, 1.1442746434363842], Fake: [4.6254612958431247, 1.3341822612201526]\n",
      "\n",
      ") \n",
      "19400: \n",
      "\tD: 0.5080984234809875/0.8742434978485107 \n",
      "\tG: 0.5182605981826782 (Real: [4.2737450283765792, 1.1798317761053367], Fake: [3.5855986568331719, 1.1808251757581381]\n",
      "\n",
      ") \n",
      "19600: \n",
      "\tD: 0.2669711709022522/0.4767753779888153 \n",
      "\tG: 0.9726938009262085 (Real: [3.9404520332813262, 1.321161923261033], Fake: [4.3613231849670413, 1.4724803953715151]\n",
      "\n",
      ") \n",
      "19800: \n",
      "\tD: 1.2249422073364258/0.4217718839645386 \n",
      "\tG: 1.3151545524597168 (Real: [4.0715565115213392, 1.0864178092402714], Fake: [5.3283413553237917, 1.4161084991570947]\n",
      "\n",
      ") \n",
      "20000: \n",
      "\tD: 0.027146780863404274/0.7456513047218323 \n",
      "\tG: 0.5517297387123108 (Real: [3.9338908898830414, 1.3377976410588399], Fake: [4.2012256681919098, 1.2627823231391324]\n",
      "\n",
      ") \n",
      "20200: \n",
      "\tD: 0.15077880024909973/0.724448561668396 \n",
      "\tG: 1.2487188577651978 (Real: [4.0671488973498349, 1.2676526060676414], Fake: [2.9718717226944862, 1.0704729512665851]\n",
      "\n",
      ") \n",
      "20400: \n",
      "\tD: 1.0303839445114136/0.3272040784358978 \n",
      "\tG: 1.5644716024398804 (Real: [4.0615675103664399, 1.0335883263831871], Fake: [5.2854558587074276, 1.4973958774483056]\n",
      "\n",
      ") \n",
      "20600: \n",
      "\tD: 0.23138955235481262/0.09846237301826477 \n",
      "\tG: 1.6620516777038574 (Real: [3.9227673244476318, 1.0997798369455574], Fake: [6.9120859122276306, 1.6363737111421326]\n",
      "\n",
      ") \n",
      "20800: \n",
      "\tD: 0.6639695763587952/0.04411774501204491 \n",
      "\tG: 2.404867649078369 (Real: [3.9226442122459413, 1.2855922319233801], Fake: [5.9915594053268428, 1.6921146889219107]\n",
      "\n",
      ") \n",
      "21000: \n",
      "\tD: 0.8377283215522766/0.47306200861930847 \n",
      "\tG: 0.6539260745048523 (Real: [4.1852242052555084, 1.3305299919813225], Fake: [4.1605407592654231, 1.148239358599376]\n",
      "\n",
      ") \n",
      "21200: \n",
      "\tD: 1.2257599830627441/0.7567362785339355 \n",
      "\tG: 1.2727676630020142 (Real: [4.021933100819588, 1.1715921760159373], Fake: [3.1196308171749116, 1.0898241395387522]\n",
      "\n",
      ") \n",
      "21400: \n",
      "\tD: 2.3264713287353516/0.7512421011924744 \n",
      "\tG: 0.6245304942131042 (Real: [4.0981457078456875, 1.4026587861021742], Fake: [4.207170608043671, 1.2538244485594112]\n",
      "\n",
      ") \n",
      "21600: \n",
      "\tD: 0.9008374810218811/0.2652956545352936 \n",
      "\tG: 1.3821117877960205 (Real: [4.069496657848358, 1.1520345907986203], Fake: [4.784109642505646, 1.3631264048643494]\n",
      "\n",
      ") \n",
      "21800: \n",
      "\tD: 0.7139845490455627/0.35229790210723877 \n",
      "\tG: 0.2720535397529602 (Real: [4.1599769926071168, 1.2044219961320699], Fake: [5.0100167059898375, 1.576296084596376]\n",
      "\n",
      ") \n",
      "22000: \n",
      "\tD: 1.6051422357559204/0.7175145745277405 \n",
      "\tG: 1.207297444343567 (Real: [3.9038313567638396, 1.2570752129262905], Fake: [4.1825001049041752, 1.1910217116665549]\n",
      "\n",
      ") \n",
      "22200: \n",
      "\tD: 0.7657179832458496/1.3241767883300781 \n",
      "\tG: 0.7872768640518188 (Real: [4.0204818063974379, 1.318503327247736], Fake: [3.4398489189147949, 0.84701266715898282]\n",
      "\n",
      ") \n",
      "22400: \n",
      "\tD: 0.49105146527290344/0.9344101548194885 \n",
      "\tG: 0.4545581340789795 (Real: [4.0627281492948528, 1.3073992963319632], Fake: [3.5521381771564484, 1.3730845502839444]\n",
      "\n",
      ") \n",
      "22600: \n",
      "\tD: 0.5997607111930847/0.96140056848526 \n",
      "\tG: 0.4439499080181122 (Real: [3.6784676313400269, 1.287936521852143], Fake: [3.391346286535263, 1.1677136677986935]\n",
      "\n",
      ") \n",
      "22800: \n",
      "\tD: 0.8306961059570312/0.7658993005752563 \n",
      "\tG: 0.5784931778907776 (Real: [3.6783352243900298, 1.1935357193618128], Fake: [4.123700387477875, 1.2717503387825329]\n",
      "\n",
      ") \n",
      "23000: \n",
      "\tD: 0.6965746879577637/0.5481836795806885 \n",
      "\tG: 0.793435275554657 (Real: [3.8870643192529677, 1.2233956133922665], Fake: [4.6328542733192446, 1.3184321800760785]\n",
      "\n",
      ") \n",
      "23200: \n",
      "\tD: 0.7120798230171204/0.6847637295722961 \n",
      "\tG: 0.7682945728302002 (Real: [4.0901843464374545, 1.1837965554136214], Fake: [4.1086192250251772, 1.2333127809162765]\n",
      "\n",
      ") \n",
      "23400: \n",
      "\tD: 0.7744458317756653/1.0823988914489746 \n",
      "\tG: 0.7696968913078308 (Real: [4.1697630035877227, 1.1581413222614652], Fake: [3.4694356942176818, 1.1465355544093327]\n",
      "\n",
      ") \n",
      "23600: \n",
      "\tD: 0.6060745716094971/0.8075464963912964 \n",
      "\tG: 0.7708129286766052 (Real: [4.023969233632088, 1.3067155180815311], Fake: [3.7542435216903685, 1.1384240246105013]\n",
      "\n",
      ") \n",
      "23800: \n",
      "\tD: 0.7538931965827942/0.6008816361427307 \n",
      "\tG: 0.6374382972717285 (Real: [4.3171281254291536, 1.2832627914324162], Fake: [4.3397582459449771, 1.3667284794838344]\n",
      "\n",
      ") \n",
      "24000: \n",
      "\tD: 0.558319091796875/0.8279922604560852 \n",
      "\tG: 0.48694783449172974 (Real: [3.8405923391133547, 1.3766249239608948], Fake: [4.261973314285278, 1.3294979784880037]\n",
      "\n",
      ") \n",
      "24200: \n",
      "\tD: 0.6733783483505249/0.7332653999328613 \n",
      "\tG: 0.6797206997871399 (Real: [3.985550504922867, 1.2940988782541853], Fake: [4.022956473827362, 1.0793126686214325]\n",
      "\n",
      ") \n",
      "24400: \n",
      "\tD: 0.6293579936027527/0.5528169870376587 \n",
      "\tG: 0.6768423318862915 (Real: [3.8446426045894624, 1.2489219168851615], Fake: [4.0064168143272401, 1.2139972742631036]\n",
      "\n",
      ") \n",
      "24600: \n",
      "\tD: 0.793465793132782/0.6294634342193604 \n",
      "\tG: 0.5738881826400757 (Real: [3.830605149269104, 1.4613397520641003], Fake: [4.1783741569519046, 1.5644877037278531]\n",
      "\n",
      ") \n",
      "24800: \n",
      "\tD: 0.5683822631835938/0.4939221739768982 \n",
      "\tG: 0.5620754361152649 (Real: [3.9742624461650848, 1.3302599574685403], Fake: [3.9938923954963683, 1.3304410760139938]\n",
      "\n",
      ") \n",
      "25000: \n",
      "\tD: 0.676751434803009/0.5356409549713135 \n",
      "\tG: 1.3306876420974731 (Real: [4.1960491943359379, 1.3102175105918217], Fake: [4.4641694259643554, 1.499202943857993]\n",
      "\n",
      ") \n",
      "25200: \n",
      "\tD: 0.8130411505699158/0.7172696590423584 \n",
      "\tG: 0.6290769577026367 (Real: [3.9747240889072417, 1.1163303610853839], Fake: [4.1082198214530941, 1.4275523481757795]\n",
      "\n",
      ") \n",
      "25400: \n",
      "\tD: 0.64924556016922/0.5781154036521912 \n",
      "\tG: 0.8371549248695374 (Real: [4.1214848804473876, 1.3140880082654292], Fake: [4.1757803893089296, 1.4570228483369745]\n",
      "\n",
      ") \n",
      "25600: \n",
      "\tD: 0.5854002833366394/0.6335069537162781 \n",
      "\tG: 0.6505358219146729 (Real: [3.8619626450538633, 1.242716617535178], Fake: [4.1937267124652866, 1.3788223974560088]\n",
      "\n",
      ") \n",
      "25800: \n",
      "\tD: 0.819282591342926/0.6340668797492981 \n",
      "\tG: 0.7065445780754089 (Real: [3.9825815665721893, 1.1269919493336489], Fake: [4.1830917167663575, 1.2891305181912851]\n",
      "\n",
      ") \n",
      "26000: \n",
      "\tD: 0.7030841112136841/0.719111979007721 \n",
      "\tG: 0.671497106552124 (Real: [4.0130784749984745, 1.2654133168639345], Fake: [3.7750041329860688, 1.1820874333017408]\n",
      "\n",
      ") \n",
      "26200: \n",
      "\tD: 0.6807417869567871/0.7615109086036682 \n",
      "\tG: 0.730817437171936 (Real: [3.8904822407662869, 1.3588761599948285], Fake: [4.2323231887817379, 1.2339872325218968]\n",
      "\n",
      ") \n",
      "26400: \n",
      "\tD: 0.6569435000419617/0.6705121397972107 \n",
      "\tG: 0.743684709072113 (Real: [4.192010487318039, 1.1836136995548192], Fake: [4.1412065935134885, 1.4216362954873776]\n",
      "\n",
      ") \n",
      "26600: \n",
      "\tD: 0.6000776886940002/0.5671903491020203 \n",
      "\tG: 0.8805528283119202 (Real: [3.890734775364399, 1.2974553376430489], Fake: [4.6746381139755249, 1.4388572478206776]\n",
      "\n",
      ") \n",
      "26800: \n",
      "\tD: 1.034220814704895/0.5838895440101624 \n",
      "\tG: 0.9860123991966248 (Real: [3.9209777766466143, 1.2127782177004802], Fake: [3.8557435023784636, 1.6139041563941923]\n",
      "\n",
      ") \n",
      "27000: \n",
      "\tD: 0.7915793657302856/0.7709490060806274 \n",
      "\tG: 0.7035437226295471 (Real: [3.9434213608503343, 1.2086748160717709], Fake: [4.1877411127090456, 1.3616813381028816]\n",
      "\n",
      ") \n",
      "27200: \n",
      "\tD: 0.5382789969444275/0.7202197909355164 \n",
      "\tG: 0.8836864829063416 (Real: [3.7867089390754698, 1.1585098327655894], Fake: [3.4372528564929961, 1.0371573636240183]\n",
      "\n",
      ") \n",
      "27400: \n",
      "\tD: 0.7257862687110901/0.5877253413200378 \n",
      "\tG: 0.5948192477226257 (Real: [4.0836483566462993, 1.2524193607446195], Fake: [5.0588239312171934, 1.4455705147937514]\n",
      "\n",
      ") \n",
      "27600: \n",
      "\tD: 0.6152100563049316/0.767953097820282 \n",
      "\tG: 0.5455437898635864 (Real: [4.2765521824359896, 1.310703995619201], Fake: [3.4802439212799072, 0.98576310448846949]\n",
      "\n",
      ") \n",
      "27800: \n",
      "\tD: 0.5297248959541321/0.7818440198898315 \n",
      "\tG: 0.43558114767074585 (Real: [4.1612167716026303, 1.0835209145559694], Fake: [3.9113569545745848, 1.3396590554431878]\n",
      "\n",
      ") \n",
      "28000: \n",
      "\tD: 0.877408504486084/0.3730115294456482 \n",
      "\tG: 0.8348484039306641 (Real: [4.0052461999654767, 1.3726824788434566], Fake: [4.9063259458541868, 1.3827594402686474]\n",
      "\n",
      ") \n",
      "28200: \n",
      "\tD: 0.728656530380249/0.9192745685577393 \n",
      "\tG: 0.5558344125747681 (Real: [4.1675916528701782, 1.2667948393620254], Fake: [3.339049218893051, 1.2669455094966713]\n",
      "\n",
      ") \n",
      "28400: \n",
      "\tD: 0.7843648195266724/0.6923267245292664 \n",
      "\tG: 0.4718838334083557 (Real: [3.9695661914348603, 1.1407185958829396], Fake: [4.7758099174499513, 1.3649878706569931]\n",
      "\n",
      ") \n",
      "28600: \n",
      "\tD: 0.7939835786819458/0.6540973782539368 \n",
      "\tG: 0.8105847239494324 (Real: [4.1492266607284547, 1.2127926904173374], Fake: [4.3690043067932125, 1.3219934479605786]\n",
      "\n",
      ") \n",
      "28800: \n",
      "\tD: 0.5378061532974243/0.7731391787528992 \n",
      "\tG: 0.565403938293457 (Real: [4.0072731769084928, 1.177579071788093], Fake: [3.3251443684101103, 1.069510455305001]\n",
      "\n",
      ") \n",
      "29000: \n",
      "\tD: 0.8158634305000305/0.6269184947013855 \n",
      "\tG: 0.6098682284355164 (Real: [4.1175059121847148, 1.1395367243797609], Fake: [4.9655970788002017, 1.5080161992869026]\n",
      "\n",
      ") \n",
      "29200: \n",
      "\tD: 0.5129578113555908/0.6691907644271851 \n",
      "\tG: 0.9035887122154236 (Real: [3.8308724725246428, 1.2590267349557172], Fake: [3.7415140628814698, 1.4457968597542965]\n",
      "\n",
      ") \n",
      "29400: \n",
      "\tD: 0.5185248851776123/0.6412976980209351 \n",
      "\tG: 0.6688947677612305 (Real: [4.0935021829605098, 1.3717047467412939], Fake: [3.8192110443115235, 1.0734190528459606]\n",
      "\n",
      ") \n",
      "29600: \n",
      "\tD: 0.7266883850097656/0.6037455201148987 \n",
      "\tG: 0.6495800018310547 (Real: [3.8296992307901383, 1.2771416117705279], Fake: [4.739962637424469, 1.5877451063172174]\n",
      "\n",
      ") \n",
      "29800: \n",
      "\tD: 0.6810862421989441/0.6967270970344543 \n",
      "\tG: 0.7623586654663086 (Real: [4.1140069806575772, 1.2520740776588455], Fake: [3.7087023901939391, 1.1809918146760638]\n",
      "\n",
      ") \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for d_index in range(d_steps):  # train how many times per epoch\n",
    "        # 1. Train D on real + fake\n",
    "        D.zero_grad()\n",
    "        \n",
    "        # 1.1 Train D on real\n",
    "        d_real_data = Variable(d_sampler(d_input_size))  # every time, create a new set of normal dist. data\n",
    "                                                        # with numpy [1,100] input size\n",
    "        d_real_decision = D(preprocess(d_real_data))  # Preprocess to add the variance in the next 100 values as [1,100]\n",
    "                                                        # means, now the input numpy array is [1,200]\n",
    "                                                        # then put into the D: Discriminator to \n",
    "        d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones is real, and all the real samples are real\n",
    "        d_real_error.backward()  # BP in Discriminator\n",
    "        \n",
    "        # 1.2 Train D on fake\n",
    "        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))  # every time, create a new set of uniform dist. data.\n",
    "                                                                        # with numpy [100, 1] input size\n",
    "        d_fake_data = G(d_gen_input).detach() # IMPORTANT: detach the G model in this Discriminator's training\n",
    "                                                # to avoid training G on these labels\n",
    "        d_fake_decision = D(preprocess(d_fake_data.t()))  # preprocess to add the variance as before, and put into the D\n",
    "        d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = fake\n",
    "        d_fake_error.backward()  # BP 2nd time in Discriminator\n",
    "        d_optimizer.step()   # Only optimize D parameters\n",
    "    \n",
    "    for g_index in range(g_steps):\n",
    "        # 2. Train G on D's reponse (but DO NOT train D on these labels)\n",
    "        G.zero_grad()\n",
    "        \n",
    "        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "        g_fake_data = G(gen_input)\n",
    "        dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "        g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))\n",
    "        \n",
    "        g_error.backward()\n",
    "        g_optimizer.step()  # Only optimize G's parameters\n",
    "        \n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"%s: \\n\\tD: %s/%s \\n\\tG: %s (Real: %s, Fake: %s\\n\\n) \" % (epoch,\n",
    "                                                           extract(d_real_error)[0],\n",
    "                                                           extract(d_fake_error)[0],\n",
    "                                                           extract(g_error)[0],\n",
    "                                                           stats(extract(d_real_data)),\n",
    "                                                           stats(extract(d_fake_data))))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.9412\n",
       " 6.1454\n",
       " 5.1307\n",
       " 6.0967\n",
       " 6.1080\n",
       " 3.2414\n",
       " 6.1028\n",
       " 2.3529\n",
       " 6.1464\n",
       " 2.6705\n",
       " 2.6215\n",
       " 5.1168\n",
       " 6.1901\n",
       " 5.4465\n",
       " 2.7076\n",
       " 2.4295\n",
       " 3.9684\n",
       " 3.1842\n",
       " 2.7246\n",
       " 5.1544\n",
       " 4.5435\n",
       " 3.2228\n",
       " 2.6096\n",
       " 3.2249\n",
       " 2.9786\n",
       " 2.5357\n",
       " 5.0398\n",
       " 3.3876\n",
       " 3.6350\n",
       " 3.9381\n",
       " 5.9401\n",
       " 2.7713\n",
       " 3.6041\n",
       " 6.1651\n",
       " 5.9866\n",
       " 2.8363\n",
       " 4.6031\n",
       " 4.3853\n",
       " 5.7740\n",
       " 2.8233\n",
       " 3.4974\n",
       " 4.0749\n",
       " 5.1216\n",
       " 4.3273\n",
       " 3.7915\n",
       " 2.9692\n",
       " 2.6277\n",
       " 2.3476\n",
       " 2.7157\n",
       " 5.1766\n",
       " 2.3707\n",
       " 2.7551\n",
       " 4.1421\n",
       " 2.7904\n",
       " 4.1393\n",
       " 2.7285\n",
       " 5.9484\n",
       " 2.6772\n",
       " 6.0633\n",
       " 2.9055\n",
       " 3.0337\n",
       " 2.7951\n",
       " 5.3271\n",
       " 2.8298\n",
       " 4.6574\n",
       " 5.8907\n",
       " 2.8626\n",
       " 4.0076\n",
       " 6.0367\n",
       " 4.9561\n",
       " 4.7573\n",
       " 3.9880\n",
       " 5.8766\n",
       " 2.3231\n",
       " 2.3066\n",
       " 3.6244\n",
       " 3.3158\n",
       " 5.6217\n",
       " 5.3326\n",
       " 2.6045\n",
       " 2.7911\n",
       " 2.8466\n",
       " 5.2243\n",
       " 2.8278\n",
       " 5.0281\n",
       " 6.1666\n",
       " 5.7426\n",
       " 3.9341\n",
       " 2.9478\n",
       " 2.3905\n",
       " 3.4379\n",
       " 5.1290\n",
       " 4.5601\n",
       " 2.7814\n",
       " 4.9065\n",
       " 2.5389\n",
       " 2.7048\n",
       " 3.9274\n",
       " 3.1769\n",
       " 2.8940\n",
       "[torch.FloatTensor of size 100x1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 2.9412  6.1454  5.1307  6.0967  6.1080  3.2414  6.1028  2.3529  6.1464  2.6705\n",
       "\n",
       "Columns 10 to 19 \n",
       " 2.6215  5.1168  6.1901  5.4465  2.7076  2.4295  3.9684  3.1842  2.7246  5.1544\n",
       "\n",
       "Columns 20 to 29 \n",
       " 4.5435  3.2228  2.6096  3.2249  2.9786  2.5357  5.0398  3.3876  3.6350  3.9381\n",
       "\n",
       "Columns 30 to 39 \n",
       " 5.9401  2.7713  3.6041  6.1651  5.9866  2.8363  4.6031  4.3853  5.7740  2.8233\n",
       "\n",
       "Columns 40 to 49 \n",
       " 3.4974  4.0749  5.1216  4.3273  3.7915  2.9692  2.6277  2.3476  2.7157  5.1766\n",
       "\n",
       "Columns 50 to 59 \n",
       " 2.3707  2.7551  4.1421  2.7904  4.1393  2.7285  5.9484  2.6772  6.0633  2.9055\n",
       "\n",
       "Columns 60 to 69 \n",
       " 3.0337  2.7951  5.3271  2.8298  4.6574  5.8907  2.8626  4.0076  6.0367  4.9561\n",
       "\n",
       "Columns 70 to 79 \n",
       " 4.7573  3.9880  5.8766  2.3231  2.3066  3.6244  3.3158  5.6217  5.3326  2.6045\n",
       "\n",
       "Columns 80 to 89 \n",
       " 2.7911  2.8466  5.2243  2.8278  5.0281  6.1666  5.7426  3.9341  2.9478  2.3905\n",
       "\n",
       "Columns 90 to 99 \n",
       " 3.4379  5.1290  4.5601  2.7814  4.9065  2.5389  2.7048  3.9274  3.1769  2.8940\n",
       "[torch.FloatTensor of size 1x100]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_fake_data.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dig Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.83379388  4.1680665   3.73416996  3.78494191  3.43783236  4.77701855\n",
      "   4.87703848  5.03326941  4.43956566  2.56148005  4.5754385   4.50966072\n",
      "   3.99340129  6.47858572  5.00684881  3.57859445  3.16589665  2.99307227\n",
      "   2.88631701  5.43812656  6.40190506  2.24437165  3.97092652  3.93438935\n",
      "   4.47829199  2.71173811  0.99943173  3.06521297  4.29873323  4.81077719\n",
      "   3.5381341   5.49740505  5.14568233  6.21595144  3.78045368  5.00694323\n",
      "   3.81046605  3.8841598   5.23519659  3.88618898  4.66366148  3.98903871\n",
      "   5.64228058  2.83580089  4.08628702  4.86716795  2.71492863  3.99117351\n",
      "   4.12422419  5.30513191  3.37135077  3.16099644  3.84489298  2.92321062\n",
      "   4.87513781  3.78819036  3.00639915  5.52395535  4.99069262  3.48879099\n",
      "   1.45460021  4.61210632  2.79096293  5.04037905  2.95161176  3.60277009\n",
      "   2.74443078  4.63886929  4.44865561  4.74550009  3.85994506  1.95604467\n",
      "   5.99011612  4.0906682   4.31995344  4.76626968  3.06933641  4.64397335\n",
      "   5.36472893  4.56793404  2.39321351  3.0536201   4.83690548  5.16094255\n",
      "   3.26970959  2.47489691  0.9990322   6.29336357  4.7446599   3.79031301\n",
      "   4.20228577  4.32160139  4.66254568  4.07062578  3.49108815  6.48133183\n",
      "   5.91200399  4.10355616  4.09286928  4.84181309  2.96228051  3.41090012\n",
      "   2.77413964  3.89566875  3.91574049  5.60057116  3.46588993  3.95527434\n",
      "   2.40596414  4.71434879  4.18309164  5.96014881  2.70646405  2.70526791\n",
      "   3.01389599  4.40635252  3.56156039  3.60524869  3.03218055  4.42530012\n",
      "   1.83814394  2.74724245  5.80731821  5.38263798  5.70408916  3.20087314\n",
      "   3.45436716  3.77311039  4.86296797  4.39792061  3.05290985  4.32457638\n",
      "   5.90179968  3.1135726   2.03551936  4.92484665  2.89379621  4.53256035\n",
      "   3.88482475  3.44118547  4.08262634  3.73241115  3.02517581  3.2329731\n",
      "   3.15772581  3.97566319  3.27370548  3.31652045  3.81688571  4.48920631\n",
      "   4.06185675  5.15947056  2.38507652  2.0835619   5.27857685  5.56206131\n",
      "   6.05710649  3.52173686  3.17915654  4.82066202  3.63001394  2.49921155\n",
      "   3.5398519   4.43529701  4.37998247  4.94106388  5.31878328  5.7733779\n",
      "   2.33939028  3.19932914  6.2230196   4.54190159  3.82080793  4.03079844\n",
      "   1.55264068  4.19764614  3.0319252   4.97176218  4.04187346  5.11739683\n",
      "   3.60588336  2.09409881  3.33827281  3.63000584  5.63966656  5.74107122\n",
      "   4.15535545  3.68257666  4.0194664   5.81850624  3.88942122  3.3606751\n",
      "   3.72485375  1.24199045  4.28916359  0.63714606  3.40196586  4.89573097\n",
      "   6.43295193  6.16285372  3.84326911  5.48876619  5.30528259  4.68148518\n",
      "   3.80934072  3.4957881   4.28717995  2.91589046  5.518116    4.42386103\n",
      "   4.80283308  0.72186559  5.01368666  3.27981138  2.3139677   4.58509302\n",
      "   2.73515272  3.9058156   4.16590738  4.78857279  4.10018301  5.37937355\n",
      "   6.40753222  6.09899044  4.49086523  3.25900292  5.6089406   2.68305159\n",
      "   2.75086594  4.02783442  3.81836534  6.58066654  4.75678587  1.79608023\n",
      "   2.15472054  3.60770941  3.91573167  2.76455331  4.84751463  3.70716238\n",
      "   3.70541263  2.36308861  2.5289669   2.91771483  4.40577984  4.4421916\n",
      "   3.5177474   5.70176935  3.62606382  2.15044236  4.06361008  2.32524014\n",
      "   2.29267406  4.13391542  4.27555418  3.30725861  2.78801203  4.36871576\n",
      "   2.78332567  4.17157602  4.22650051  1.66778433  2.47273707  0.97775733\n",
      "   4.36658096  2.49290466  2.4699893   2.77914476  3.31258178  3.76708341\n",
      "   4.29865837  2.99983048  5.47016668  4.47161961  4.87621117  4.35262585\n",
      "   4.72179031  3.9861691   3.07253718  2.70452785  3.73948121  2.46742535\n",
      "   4.06401539  3.18386483  3.45972252  3.96759558  3.61816883 -0.5837943\n",
      "   3.30504775  4.07705402  6.16202307  4.5244112   5.53631973  3.16023445\n",
      "   2.57922316  1.91150486  3.18773413  4.13751745  4.87447929  2.00947952\n",
      "   1.21637285  4.58218718  5.1854372   2.96889091  4.2420125   6.00944757\n",
      "   5.08440018  4.90682602  5.59720802  4.94301033  4.04303122  1.98355007\n",
      "   3.04469085  5.50768661 -0.39061382  5.18920231  5.60276508  1.79399741\n",
      "   4.2762742   3.94848013  3.81632447  3.31813741  4.31716347  3.34465742\n",
      "   3.14383173  6.13715887  3.59808731  5.63000488  2.96149659  4.73548508\n",
      "   2.206599    2.41602349  2.27562809  5.0706172   1.62802482  5.26030779\n",
      "   4.05450487  5.00519371  3.58749819  2.85815334  2.43254924  5.20255613\n",
      "   5.86406755  2.2893095   3.2123847   2.60891199  4.89648533  6.82650471\n",
      "   4.350492    4.80711031  2.12890339  2.37407899  4.19712019  3.12911844\n",
      "   4.02758455  5.48055601  6.83418179  2.38917255  3.76434231  3.44702792\n",
      "   4.61675835  2.14375591  3.77919507  4.40775585  4.30857515  6.78731251\n",
      "   4.21089029  4.89105225  2.8871417   2.25383043  6.08139038  4.09131432\n",
      "   4.96499491  4.27585411  4.84267473  4.38540983  4.39854336  3.87785292\n",
      "   3.65581632  4.22954988  4.06990099  2.95050621  5.28767872  3.00080061\n",
      "   2.71050644  4.08396816  4.34570694  4.25978947  3.53299737  3.61128259\n",
      "   5.83240509  3.98530126  6.30619669  3.17460561  4.73431969  6.20208073\n",
      "   2.42046452  3.63849211  4.32446766  5.93309116  6.94927406  2.06195593\n",
      "   2.98438954  5.79375505  5.11896515  3.5585103   4.71493864  3.85533285\n",
      "   1.955387    3.72476459  5.60193872  3.18802857  4.24264431  5.36092091\n",
      "   5.04224396  0.96350718  2.48833466  3.51773977  3.581949    5.36975622\n",
      "   2.32302499  4.23693037  4.29478264  4.85410738  6.10626221  2.94125867\n",
      "   2.16666746  3.2636795   4.50365973  0.99654466  5.10027313  3.12929535\n",
      "   4.51301861  3.33517909  3.33129287  2.88463664  6.12170601  3.93049788\n",
      "   1.53913355  4.59985542  1.13224018  5.28535032  4.92536402  4.50317812\n",
      "   4.73928022  4.25041008  2.94801259  3.29138303  2.68057323  4.71002483\n",
      "   2.55837178  5.12659979  5.81890392  4.23000002  1.09403944  2.38045526\n",
      "   4.43878174  3.45726037  5.14306545  6.36049175  4.37034178  3.90085053\n",
      "   3.70801425  2.53606963  4.64305973  4.52029133  4.86896324  5.44573689\n",
      "   2.41532564  3.30485678  5.05674744  3.55076599  3.24955773  5.13839769\n",
      "   2.43912864  4.20972347  3.6101017   5.25414181  2.71566296  5.76034403\n",
      "   3.39901567  4.84967995  3.15694284  3.92928958  6.02350903  2.68352222\n",
      "   5.15997648  3.8282485   5.02700186  5.41527224  4.27779961  4.07825661\n",
      "   3.20065498  4.80219746  3.42037964  2.52778864  3.17950583  4.30728197\n",
      "   3.41045427  5.03206253  3.39366293  2.95668745  5.57953453  3.79072857\n",
      "   4.7064209   5.26878595  5.92057753  6.15325546  3.42209601  5.0980916\n",
      "   4.40295172  5.99366617  2.24113488  5.69935179  4.25987816  3.11815691\n",
      "   6.02209711  4.07030916  6.12358379  3.14623308  4.46808434  2.81683159\n",
      "   4.8552742   6.39046144  4.14204168  3.40798426  4.00191689  2.78765368\n",
      "   2.37671995  4.16602135  4.47316647  6.86661196  4.40179777  3.62570357\n",
      "   4.43743277  4.59731531  2.96355152  4.92840242  1.98743427  5.32232666\n",
      "   2.20819378  6.23687267  2.42955661  3.53673792  2.95620894  2.03242111\n",
      "   2.56991386  4.25551319  5.91217518  3.66561341  4.13994694  2.38369823\n",
      "   6.0152297   3.12844253  5.67512846  3.83314919  3.39476371  3.86662698\n",
      "   3.99813175  4.16113043  3.35682273  3.85962272  5.64581013  1.85727072\n",
      "   3.44736814  6.05423594  3.02936459  2.17537737  1.26530612  6.26257801\n",
      "   2.95280409  4.91098118  4.68395138  2.33933711  4.84050989  3.15352559\n",
      "   3.18019581  4.84339142  4.08640671  4.28442478  5.78793097  3.32641506\n",
      "   4.9374733   3.00748587  1.08278584  6.22908354  5.99989986  2.70290828\n",
      "   2.21638489  4.12286139  2.85042882  3.52418637  3.01135993  3.08548498\n",
      "   3.74798965  4.08939695  4.71280909  3.51102519  3.4339602   2.88737273\n",
      "   4.77243805  3.0863378   2.08807349  5.53416395  4.22611046  4.40489435\n",
      "   4.20956945  3.53627825  4.74871731  2.47721553  3.7423594   1.3584832\n",
      "   4.56013918  2.96917439  3.26482177  2.70129752  4.1088829   2.65953279\n",
      "   3.56364989  4.685112    3.44727516  4.51088953  2.32263875  5.82412767\n",
      "   2.60631728  1.97419631  4.67184544  3.31284118  5.90663862  3.5393157\n",
      "   2.7449398   4.77486515  2.24837685  5.51104259  4.41560936  4.53938532\n",
      "   2.28945613  3.36691833  4.30460024  5.13752174  2.61798596  5.71124172\n",
      "   4.62190437  4.59417152  5.40517807  2.52025628  5.97938395  4.65902662\n",
      "   5.41657066  3.12542462  4.03600454  3.32138395  5.5284214   5.84079933\n",
      "   4.77292633  5.2316041   4.31771517  5.96853685  3.91396379  2.59140873\n",
      "   4.33355331  0.91994548  4.84265709  3.89006138  5.42085123  6.21466684\n",
      "   4.05597973  1.30050695  2.37829232  1.5870502   2.85054755  3.43369913\n",
      "   3.92894387  5.56800318  3.95568228  4.36079168  4.04416561  3.92334723\n",
      "   2.34669209  2.736408    2.89970446  2.14219379  5.76930237  3.41756129\n",
      "   2.14269757  4.03489161  5.57104301  4.10147047  2.84170341  4.62391233\n",
      "   2.54134893  4.14205265  3.94782925  3.73853827  5.48544741  5.89988136\n",
      "   3.89163756  2.8894105   3.63321114  3.41354632  4.42717791  5.34910393\n",
      "   5.09402609  2.98559809  2.99625087  3.51112008  4.11522388  4.7820034\n",
      "   4.84717703  5.74437141  1.47585368  4.19658375  3.59171367  1.38406205\n",
      "   3.43819284  2.45800972  4.44173813  3.07539535  4.09322119  3.84877706\n",
      "   5.30687141  2.7432549   2.59397721  1.65043199  2.57713509  4.06711531\n",
      "   5.49922419  1.96110797  5.51175642  3.22713995  6.11011028  3.16629291\n",
      "   6.01975965  3.77541614  3.31137228  3.71059108  2.93324137  4.36409807\n",
      "   3.62978005  2.60451531  3.13652325  4.29756403  3.70405984  1.46707141\n",
      "   4.40003395  3.4616704   2.07051682  3.33661151  2.09829283  4.50649166\n",
      "   4.3673377   2.56399202  4.66400528  6.28477669  4.17537642  2.46675372\n",
      "   3.86139321  4.72140408  3.75224447  4.07717705  3.48728204  4.20923948\n",
      "   4.13359356  5.11183739  5.45726395  1.63382804  5.04333639  3.54401445\n",
      "   5.71642828  4.13674641  3.69238281  4.76155996  4.54111719  4.82953835\n",
      "   5.37679291  3.69712591  3.70412517  4.0531106   3.49506664  3.81217432\n",
      "   4.99069023  4.15640974  2.54633117  4.5296011   2.91414046  3.82908225\n",
      "   4.11265278  3.40634489  5.01157618  3.32586527  2.79152536  5.07591629\n",
      "   5.22989464  1.81151938  3.57879877  4.66396379  4.28185081  4.31442833\n",
      "   5.55135632  4.91099977  3.81947947  5.10261965  5.18770695  4.09350538\n",
      "   5.57667637  3.79879737  3.73982573  6.36909485  4.15979147  6.38898754\n",
      "   3.3167994   3.9810431   2.51029444  3.00094175  4.31132317  5.91099024\n",
      "   3.47831297  4.66363907  4.31847334  4.49010038  6.24157333  3.72870874\n",
      "   2.08964801  3.62419128  3.30678415  4.73969793  3.20773792  2.28066707\n",
      "   2.14068818  3.54066396  5.21644258  5.05082273  4.60073376  4.23300314\n",
      "   2.92256379  4.11766434  3.45670581  4.62085104  5.17881823  3.27198195\n",
      "   4.32620525  3.96790099  1.70268512  5.02676344  3.87000728  6.63451385\n",
      "   5.75173473  3.79766893  4.00417471  4.68815756  2.64909053  1.69852412\n",
      "   4.41640091  5.75556946  4.77394724  4.05060387  3.4994061   2.781003\n",
      "   4.31618643  4.62720633  4.5191164   4.49143648  1.23260844  4.16145754\n",
      "   3.26928806  2.62838626  3.99772978  4.39193439  4.97397041  2.19667506\n",
      "   4.58648682  4.05992031  5.88679314  5.59754562  2.20102668  4.36803865\n",
      "   2.31030941  7.03567648  4.56764698  2.70034099  5.23813343  5.47042751\n",
      "   5.22632408  2.1737113   5.37294292  3.66298079  4.77566099  5.32893562\n",
      "   2.82898307  4.4401989   2.92576194  6.32636976  4.52243757  2.83996367\n",
      "   2.78271222  2.05439281  5.26048517  2.71264625  4.26896524  5.48859406\n",
      "   6.21293831  4.21267796  3.51267552  3.34259462  3.47425842  4.23759937\n",
      "   4.60905647  3.94629216  2.53198576  1.13438702  3.9907918   3.40351009\n",
      "   1.82625532  4.64090586  4.45962     4.60339355  4.60455751  4.62417841\n",
      "   2.9691906   4.32819366  4.38568306  3.963624    4.08853245  3.31686759\n",
      "   5.26110172  2.57626963  3.40163994  2.69503975  3.89117002  3.13161969\n",
      "   2.36550832  4.92426682  5.52395248  3.02891493  4.91272593  3.18077564\n",
      "   6.01783609  2.94626498  4.00391769  5.67249918  5.22703981  4.56616974\n",
      "   4.32475805  4.59582949  1.67698181  5.25697756  5.33634043  6.13945961\n",
      "   5.29330492  3.4920001   2.97034621  2.74042106  3.43966532  2.72656584\n",
      "   4.59146309  4.86867237  5.52748871  4.48592091  3.84473157  4.42428732\n",
      "   4.69033623  4.72647476  4.24891996  5.73100615  2.75439262  5.81312704\n",
      "   4.45563889  4.44630814  1.72030389  3.12215066  4.13957787  1.50023758\n",
      "   3.26040244  3.66553378  2.12047887  2.35935354  2.76915145  5.97444344\n",
      "   3.99319148  2.39546943  3.82925987  5.68030882  3.81325769  4.36582422\n",
      "   4.7395587   6.43271732  4.08381653  4.93553638  3.62661409  4.59983301\n",
      "   5.2409339   4.87121868  3.79183984  3.68552065  5.29998302  2.34889698\n",
      "   2.20483899  2.05760956  5.80628157  3.27085805]]\n",
      "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
      "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])]\n",
      "(1, 100)\n",
      "(1, 1000)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAGi1JREFUeJzt3X+QZWV95/H3hyCMkMxQOuugMbNi0Nm2snEzTSCUAd2M\nrgq1aOJWQscpVqnEZf1R1qxbMe5KQKiNiivDorJlla5KJnaKxbL8UQNoUIngj0kYf0RtxtVAWmQY\nvaADxdj8mHn2j3MamuYZ6Hv79pzbPe9X1S3oc557+vvM7b79uc/znHNSSkGSJGm+I7ouQJIkjSZD\ngiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmq6iskJHlbkh1J\n7kmyJ8knkzx3XpuPJDkw77F9Xpujk3wgSS/JvUmuTvK0YXRIkiQNR78jCacB7wNOAV4MPAn4XJIn\nz2t3DbAOOL59TMzbfxlwJvAq4HTgGcAn+qxFkiQtoSzmBk9J1gI/AU4vpdzYbvsIsKaU8gcHec5q\n4KfA2aWUT7bbNgBTwO+UUnYMXJAkSRqaxa5JOA4owN3ztr+onY64JckVSZ4yZ984cCRw/eyGUsou\nYBo4dZH1SJKkITly0CcmCc20wY2llO/N2XUNzdTBrcCvA+8Etic5tTTDFscDD5RS7pl3yD3tvtr3\neirwUuA2YGbQmiVJOgytAp4FXFdKuaufJw4cEoArgOcBL5i7sZRy1Zwvv5vkH4EfAi8Cvjjg93op\n8NcDPleSJMGrgY/384SBQkKS9wNnAKeVUnY/XttSyq1JesCJNCHhTuCoJKvnjSasa/fV3Aawbds2\nxsbGBil55GzZsoWtW7d2XcbQrKT+rKS+wMrqz+7duzn//PN5y1ve0nUpj3Hcccfx9Kc/va/nrKTX\nBuzPqJqammLz5s3Q/i3tR98hoQ0IrwBeWEqZXkD7ZwJPBWbDxM3AQ8AmYO7CxfXAVw9ymBmAsbEx\nNm7c2G/JI2nNmjUrpi+wsvqzkvoCK6c/09PTvOAFpzEzs2/2DW+krFp1DLt2TbF+/foFP2elvDaz\n7M/I63u6vq+QkOQKmtMZzwLuS7Ku3bW3lDKT5FjgApo1CXfSjB68G/g+cB1AKeWeJB8GLk3yM+Be\n4HLgJs9skHQwvV6PmZl9wG8BH+q6nHmmmJnZTK/X6yskSKOu35GE82jOZvjSvO2vBa4E9gO/CZxD\nc+bDHTTh4C9KKQ/Oab+lbXs1cDRwLfCGPmuRdFj6FWBFfbqTRlZfIaGU8rinTJZSZoCXLeA49wNv\nah+SJGkEee+GjkxMzL8I5fK2kvqzkvoCK68/zclOK8NKe23sz8qzqCsuHipJNgI333zzzSttEYmk\nBdq5cyfj4+M0a59H7X1gJzCO71EaRY/87jBeStnZz3MdSZAkSVWGBEmSVGVIkCRJVYYESZJUZUiQ\nJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJ\nVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWG\nBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJ\nklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUdWTXBUgaLdPT0/R6va7LeIypqamuS5AOO4YESQ+bnp5m\nw4YxZmb2dV2KpBFgSJD0sF6v1waEbcBY1+XMsx04v+sipMOKIUFSxRiwsesi5nG6QTrUXLgoSZKq\nDAmSJKnKkCBJkqoMCZIkqcqQIEmSqvoKCUnelmRHknuS7EnyySTPrbS7KMkdSfYl+XySE+ftPzrJ\nB5L0ktyb5OokT1tsZyRJ0vD0O5JwGvA+4BTgxcCTgM8lefJsgyRvBd4IvA44GbgPuC7JUXOOcxlw\nJvAq4HTgGcAnBuyDJElaAn1dJ6GUcsbcr5O8BvgJMA7c2G5+M3BxKeWzbZtzgD3AK4GrkqwGzgXO\nLqXc0LZ5LTCV5ORSyo7BuyNJkoZlsWsSjgMKcDdAkhOA44HrZxuUUu4Bvg6c2m46iSaczG2zC5ie\n00aSJHVs4JCQJDTTBjeWUr7Xbj6eJjTsmdd8T7sPYB3wQBseDtZGkiR1bDGXZb4CeB7wgiHV8oS2\nbNnCmjVrHrVtYmKCiYmJQ1WCJEkja3JyksnJyUdt27t378DHGygkJHk/cAZwWill95xddwKhGS2Y\nO5qwDvjGnDZHJVk9bzRhXbvvoLZu3crGjaN2PXlJkkZD7YPzzp07GR8fH+h4fU83tAHhFcC/LaVM\nz91XSrmV5g/9pjntV9OcDfGVdtPNwEPz2mwA1gNf7bceSZK0NPoaSUhyBTABnAXcl2Rdu2tvKWWm\n/f/LgLcn+QFwG3AxcDvwKWgWMib5MHBpkp8B9wKXAzd5ZoMkSaOj3+mG82gWJn5p3vbXAlcClFIu\nSXIM8EGasx++DLy8lPLAnPZbgP3A1cDRwLXAG/otXpIkLZ1+r5OwoOmJUsqFwIWPs/9+4E3tQ5Ik\njSDv3SBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSp\nypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQ\nIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJ\nkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKq\njuy6AOlwMz09Ta/X67qMqqmpqa5LkDRCDAnSITQ9Pc2GDWPMzOzruhRJekKGBOkQ6vV6bUDYBox1\nXU7FduD8rouQNCIMCVInxoCNXRdR4XSDpEe4cFGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIk\nSVJV3yEhyWlJPp3kx0kOJDlr3v6PtNvnPrbPa3N0kg8k6SW5N8nVSZ622M5IkqThGWQk4Vjgm8Dr\ngXKQNtcA64Dj28fEvP2XAWcCrwJOB54BfGKAWiRJ0hLp+2JKpZRrgWsBkuQgze4vpfy0tiPJauBc\n4OxSyg3tttcCU0lOLqXs6LcmSZI0fEu1JuFFSfYkuSXJFUmeMmffOE04uX52QyllFzANnLpE9UiS\npD4txWWZr6GZOrgV+HXgncD2JKeWUgrN9MMDpZR75j1vT7tPkiSNgKGHhFLKVXO+/G6SfwR+CLwI\n+OJijr1lyxbWrFnzqG0TExNMTMxf8iBJ0uFncnKSycnJR23bu3fvwMdb8hs8lVJuTdIDTqQJCXcC\nRyVZPW80YV2776C2bt3Kxo2jeFMcSZK6V/vgvHPnTsbHxwc63pJfJyHJM4GnArvbTTcDDwGb5rTZ\nAKwHvrrU9UiSpIXpeyQhybE0owKzZzY8O8nzgbvbxwU0axLubNu9G/g+cB1AKeWeJB8GLk3yM+Be\n4HLgJs9skCRpdAwy3XASzbRBaR/vbbd/jObaCb8JnAMcB9xBEw7+opTy4JxjbAH2A1cDR9OcUvmG\nAWqRpJExNTXVdQkHtXbtWtavX991GVpmBrlOwg08/jTFyxZwjPuBN7UPSVrmdgNHsHnz5q4LOahV\nq45h164pg4L6suQLFyVp5fs5cADYBox1XEvNFDMzm+n1eoYE9cWQIElDMwZ4BpZWDu8CKUmSqgwJ\nkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIk\nqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnK\nkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAg\nSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmS\nqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqvoOCUlOS/LpJD9OciDJWZU2FyW5I8m+\nJJ9PcuK8/Ucn+UCSXpJ7k1yd5GmL6YgkSRquQUYSjgW+CbweKPN3Jnkr8EbgdcDJwH3AdUmOmtPs\nMuBM4FXA6cAzgE8MUIskSVoiR/b7hFLKtcC1AElSafJm4OJSymfbNucAe4BXAlclWQ2cC5xdSrmh\nbfNaYCrJyaWUHQP1RJIkDdVQ1yQkOQE4Hrh+dlsp5R7g68Cp7aaTaMLJ3Da7gOk5bSRJUseGvXDx\neJopiD3ztu9p9wGsAx5ow8PB2kiSpI71Pd3QpS1btrBmzZpHbZuYmGBiYqKjiiRJGh2Tk5NMTk4+\natvevXsHPt6wQ8KdQGhGC+aOJqwDvjGnzVFJVs8bTVjX7juorVu3snHjxiGWK0nSylH74Lxz507G\nx8cHOt5QpxtKKbfS/KHfNLutXah4CvCVdtPNwEPz2mwA1gNfHWY9kiRpcH2PJCQ5FjiRZsQA4NlJ\nng/cXUr5Ec3pjW9P8gPgNuBi4HbgU9AsZEzyYeDSJD8D7gUuB27yzAZJkkbHINMNJwFfpFmgWID3\ntts/BpxbSrkkyTHAB4HjgC8DLy+lPDDnGFuA/cDVwNE0p1S+YaAeSJKkJTHIdRJu4AmmKUopFwIX\nPs7++4E3tQ9JkjSCvHeDJEmqWlanQEqSBjc1NdV1CVVr165l/fr1XZehCkOCJK14u4Ej2Lx5c9eF\nVK1adQy7dk0ZFEaQIUGSVryfAweAbcBYx7XMN8XMzGZ6vZ4hYQQZEiTpsDEGeEE6LZwLFyVJUpUh\nQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKV927Q\nijQ9PU2v1+u6jMcY1Vv1SlKNIUErzvT0NBs2jDEzs6/rUiRpWTMkaMXp9XptQBjF2+JuB87vughJ\nWhBDglawUbwtrtMNkpYPFy5KkqQqQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4Ik\nSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmq\nMiRIkqQqQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4IkSaoyJEiSpCpDgiRJqjIk\nSJKkKkOCJEmqMiRIkqSqoYeEJBckOTDv8b15bS5KckeSfUk+n+TEYdchSZIWZ6lGEr4DrAOObx+/\nO7sjyVuBNwKvA04G7gOuS3LUEtUiSZIGcOQSHfehUspPD7LvzcDFpZTPAiQ5B9gDvBK4aonqkSRJ\nfVqqkYTnJPlxkh8m2Zbk1wCSnEAzsnD9bMNSyj3A14FTl6gWSZI0gKUICV8DXgO8FDgPOAH4uyTH\n0gSEQjNyMNeedp8kSRoRQ59uKKVcN+fL7yTZAfwz8IfALYs59pYtW1izZs2jtk1MTDAxMbGYw0qS\ntCJMTk4yOTn5qG179+4d+HhLtSbhYaWUvUm+D5wIfAkIzaLGuaMJ64BvPNGxtm7dysaNG5eiTEmS\nlr3aB+edO3cyPj4+0PGW/DoJSX6ZJiDcUUq5FbgT2DRn/2rgFOArS12LJElauKGPJCR5D/AZmimG\nXwXeATwI/E3b5DLg7Ul+ANwGXAzcDnxq2LVIkqTBLcV0wzOBjwNPBX4K3Aj8TinlLoBSyiVJjgE+\nCBwHfBl4eSnlgSWoRZIkDWgpFi4+4SrCUsqFwIXD/t6SJGl4vHeDJEmqMiRIkqQqQ4IkSaoyJEiS\npCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQq\nQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqOrLrArQ8TU9P0+v1ui6jampqqusSJGlFMCSob9PT\n02zYMMbMzL6uS5EkLSFDgvrW6/XagLANGOu6nIrtwPldFyFJy54hQYswBmzsuogKpxskaRhcuChJ\nkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqPLtBktS5Ub0I2tq1a1m/fn3XZXTGkCBJ6tBu4Ag2b97c\ndSFVq1Ydw65dU4dtUDAkSJI69HPgAKN5cbYpZmY20+v1DAmSJHVnVC/Odnhz4aIkSaoyJEiSpCpD\ngiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4Ik\nSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQqQ0JHJicnuy5hyFZSf1ZSX2Dl9efargsYopX2\n2qy0/qykn7XBdBoSkrwhya1JfpHka0l+u8t6DiVDwihbSX2Bldef67ouYIhW2muz0vqzkn7WBnNk\nV984yR8B7wVeB+wAtgDXJXluKaXXVV2jZHp6ml5v9P4ppqamui5BknQIdBYSaELBB0spVwIkOQ84\nEzgXuGSpv3kphQsvfAc33njTUn+rqm9/+5ts2vSSg+6fmZnh7//+H3jwwZlDWJUkSY/oJCQkeRIw\nDvzl7LZSSknyt8Cph6KGAwcOcNFF7wB+GzjhUHzLeY7iC194yuPsvwGYAbYBY4empAXbDpzfdRGS\npCXW1UjCWuCXgD3ztu8BNlTar4LhDnPv37+//b8x4AVDO+7C3QJsepz936H557j10JTTlzva/24H\nZl+T24G/7qacx5gdHZpbXz+Wsi+LrW0Q/fSni/oWara2PYzOz9qsQf/dDtXvzaF6XQfpzyj/zDV/\nopb7FOuc+lf1+9yUUoZbzUK+afJ04MfAqaWUr8/Z/m7g9FLKqfPa/zGj964gSdJy8upSysf7eUJX\nIwk9YD+wbt72dcCdlfbXAa8GbqMZg5ckSQuzCngWA5yu0clIAkCSrwFfL6W8uf06wDRweSnlPZ0U\nJUmSHtbl2Q2XAh9NcjOPnAJ5DPDRDmuSJEmtzkJCKeWqJGuBi2imGb4JvLSU8tOuapIkSY/obLpB\nkiSNNu/dIEmSqgwJkiSpatmFhCT/LclNSe5LcnfX9fRrJd3UKslpST6d5MdJDiQ5q+uaBpXkbUl2\nJLknyZ4kn0zy3K7rGkSS85J8K8ne9vGVJC/ruq5hSfLn7c/bpV3XMogkF7T1z318r+u6BpXkGUn+\nKkkvyb72Z29j13UNon1vnv/aHEjyvq5rG0SSI5JcnOSf2tfmB0ne3s8xll1IAJ4EXAX8764L6dec\nm1pdAPwW8C2am1qt7bSwwR1Ls+D09cByX9xyGvA+4BTgxTQ/Z59L8uROqxrMj4C3AhtpLn/+BeBT\nSUbt+t59a0P162h+d5az79As2D6+ffxut+UMJslxNJdMvB94Kc0lbN8C/KzLuhbhJB55TY4HXkLz\n3nZVl0Utwp8D/4nmPfpfAX8G/FmSNy70AMt24WKS/whsLaU83g0QRspBrg3xI5prQyz5Ta2WUpID\nwCtLKZ/uupZhaIPbT2iuAHpj1/UsVpK7gP9aSvlI17UMKskvAzcD/5nm5iHfKKX8l26r6l+SC4BX\nlFKW5aftuZK8i+bKuS/supalkOQy4IxSynIdVfwMcGcp5U/nbLsa2FdKOWchx1iOIwnL0pybWl0/\nu600Ce2Q3dRKfTmO5hPEspvSmqsdbjyb5hokX+26nkX6APCZUsoXui5kCJ7TTtP9MMm2JL/WdUED\n+vfAPyS5qp2m25nkT7ouahja9+xXAx/uupZF+AqwKclzAJI8n+ZmRdsXeoAuL6Z0uOn3plbqSDvC\ncxlwYyllWc4VJ/kNmlCwCrgX+P1Syi3dVjW4Nuj8G5rh4OXua8BrgF3A04ELgb9L8hullPs6rGsQ\nz6YZ2Xkv8D+Ak4HLk9xfSvmrTitbvN8H1gAf67qQRXgXsBq4Jcl+moGB/15K+ZuFHmAkQkKSd9LM\noR5MAcZKKd8/RCXp8HYF8Dy6uT3osNwCPJ/mTe4/AFcmOX05BoUkz6QJbS8upTzYdT2LVUqZe/38\n7yTZAfwz8IfAcpsOOgLYUUqZvXf8t9qAeh6w3EPCucA1pZTa/YSWiz8C/hg4G/geTdD+X0nuWGiI\nG4mQAPxPnviX458ORSFLqN+bWqkDSd4PnAGcVkrZ3XU9gyqlPMQjvzPfSHIy8GaaT33LzTjwL4Cd\n7SgPNKNyp7cLsI4uy3VxFVBK2Zvk+8CJXdcygN089v7OU8AfdFDL0CRZT7OA+ZVd17JIlwDvLKX8\n3/br7yZ5FvA2FhjiRiIklFLuAu7quo6lVEp5sL1PxSbg0/DwsPYm4PIua1OjDQivAF5YSpnuup4h\nOwI4uusiBvS3wL+et+2jNH+M3rWcAwI8vCDzRODKrmsZwE08drp0A83IyHJ2Ls1U8ILn7kfUMTQf\nTuc6QB/rEUciJPSjXeDzFOBfAr/ULsQA+MEymM9bUTe1SnIszZvb7Ke7Z7evx92llB91V1n/klwB\nTABnAfclmR3x2VtKWVa3J0/yl8A1NHdV/RWaxVcvBP5dl3UNqv29ftTakCT3AXeVUuZ/ih15Sd4D\nfIbmD+mvAu8AHgQmu6xrQFuBm5K8jeY0wVOAPwH+9HGfNcLaD2+vAT5aSjnQcTmL9Rng7UluB75L\nc1r0FuBDCz5CKWVZPWimJfZXHqd3XdsC6389cBvwC5qFZSd1XdMi+vJCmlQ6/7X4P13XNkBfav3Y\nD5zTdW0D9OVDNFMNv6CZyvoc8Htd1zXkPn4BuLTrOgasfRK4vX19poGPAyd0Xdci+nMG8G1gX/uH\n6Nyua1pkf17S/u6f2HUtQ+jLsTQfTm8F7gP+H00oPXKhx1i210mQJElLy+skSJKkKkOCJEmqMiRI\nkqQqQ4IkSaoyJEiSpCpDgiRJqjIkSJKkKkOCJEmqMiRIkqQqQ4IkSaoyJEiSpKr/D+1CcLtu0DpI\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110178f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_np = d_sampler(1000).numpy()\n",
    "\n",
    "print(sample_np)\n",
    "np.size(sample_np)\n",
    "\n",
    "a_np = sample_np.copy()\n",
    "a_x = [np.arange(100)]\n",
    "\n",
    "print(a_x)\n",
    "print(np.shape(a_x))\n",
    "print(np.shape(a_np))\n",
    "\n",
    "plt.hist(np.transpose(a_np)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29999: \n",
      "\tD: 0.5531200170516968/0.717462420463562 \n",
      "G: 0.7971842288970947 \n",
      "(Real: [4.1114702498912807, 1.1424093387516128], Fake: [3.9775783205032349, 1.2874585026157108]) \n"
     ]
    }
   ],
   "source": [
    "print(\"%s: \\n\\tD: %s/%s \\nG: %s \\n(Real: %s, Fake: %s) \" % (epoch,\n",
    "                                                           extract(d_real_error)[0],\n",
    "                                                           extract(d_fake_error)[0],\n",
    "                                                           extract(g_error)[0],\n",
    "                                                           stats(extract(d_real_data)),\n",
    "                                                           stats(extract(d_fake_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Comparison"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decorate_with_diffs(data, exponent):\n",
    "    mean = torch.mean(data.data, 1)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    return torch.cat([data, diffs], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean = torch.mean(d_real_data.data, 1)\n",
    "# print(d_real_data.data)\n",
    "# print(torch.mul(torch.ones(d_real_data.size()), mean.tolist()[0][0]))\n",
    "\n",
    "# print(torch.cat([d_real_data, torch.pow(d_real_data - Variable(torch.mul(torch.ones(d_real_data.size()), mean.tolist()[0][0])), 2.0)], 1))\n",
    "# print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(preprocess(d_real_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
