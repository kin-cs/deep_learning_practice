{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://github.com/devnag/pytorch-generative-adversarial-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Data [Data and Variances]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# data params\n",
    "\n",
    "data_mean = 4\n",
    "data_sd = 1.25\n",
    "\n",
    "# Model params\n",
    "g_input_size = 1 # input a random noise\n",
    "g_hidden_size = 100\n",
    "g_output_size = 1\n",
    "\n",
    "d_input_size = 100 # input minibatch\n",
    "d_hidden_size = 50\n",
    "d_output_size = 1 # Real or Fake\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "d_learning_rate =2e-5\n",
    "g_learning_rate =2e-5\n",
    "optim_beta = (0.9, 0.999)\n",
    "num_epochs = 30000\n",
    "print_interval = 200\n",
    "d_steps = 1  # training rate on d\n",
    "g_steps = 1\n",
    "\n",
    "# ### Uncomment only one of these\n",
    "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "\n",
    "(name, preprocess, d_input_func) = (\"Data and Variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x*2)\n",
    "\n",
    "print('Using Data [%s]' % (name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Data: target data + generator's random input data\n",
    "\n",
    "def get_distribution_sampler(mu, stigma):  # Normal Distribution Target Data\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, stigma, (1, n)))\n",
    "\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)\n",
    "\n",
    "###### Models: g model and d model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        x = self.map3(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        x = F.sigmoid(self.map3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]\n",
    "\n",
    "def decorate_with_diffs(data, exponent):\n",
    "    mean = torch.mean(data.data, 1)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    return torch.cat([data, diffs], 1)\n",
    "\n",
    "d_sampler = get_distribution_sampler(data_mean, data_sd)\n",
    "gi_sampler = get_generator_input_sampler()\n",
    "\n",
    "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_beta)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "\tD: 0.960450291633606/0.6117495894432068 \n",
      "\tG: 0.7815150022506714 (Real: [4.109435591697693, 1.0478115699226633], Fake: [0.25096395224332807, 0.011216928543516157]\n",
      "\n",
      ") \n",
      "200: \n",
      "\tD: 0.24724139273166656/0.7340779304504395 \n",
      "\tG: 0.6575086116790771 (Real: [3.8750937211513521, 1.2296328841284401], Fake: [0.7159476542472839, 0.031099033110379898]\n",
      "\n",
      ") \n",
      "400: \n",
      "\tD: 0.24376854300498962/0.7516789436340332 \n",
      "\tG: 0.6405978202819824 (Real: [3.9589553463459013, 1.3156933663638324], Fake: [1.2715883874893188, 0.062331629218602087]\n",
      "\n",
      ") \n",
      "600: \n",
      "\tD: 0.20586958527565002/0.5891590714454651 \n",
      "\tG: 0.8132107257843018 (Real: [3.9610462963581083, 1.1473354440627435], Fake: [1.6492239820957184, 0.093218931104043903]\n",
      "\n",
      ") \n",
      "800: \n",
      "\tD: 0.27493947744369507/0.41207045316696167 \n",
      "\tG: 1.0880851745605469 (Real: [4.1773660135269166, 1.1900776623403129], Fake: [1.8006523990631103, 0.14072002889587421]\n",
      "\n",
      ") \n",
      "1000: \n",
      "\tD: 0.19012635946273804/0.31591174006462097 \n",
      "\tG: 1.284720540046692 (Real: [4.0574538165330889, 1.0800046266069072], Fake: [1.5559862136840821, 0.21174985795152465]\n",
      "\n",
      ") \n",
      "1200: \n",
      "\tD: 0.06882192194461823/0.34648439288139343 \n",
      "\tG: 1.3191043138504028 (Real: [4.0756808400154112, 1.2994309306639902], Fake: [0.90619248449802403, 0.31041370163685755]\n",
      "\n",
      ") \n",
      "1400: \n",
      "\tD: 0.0537603422999382/0.7824702262878418 \n",
      "\tG: 0.650105357170105 (Real: [4.1316613686084747, 1.1417492239518392], Fake: [-0.17250955022871495, 0.30110274042896173]\n",
      "\n",
      ") \n",
      "1600: \n",
      "\tD: 0.0019200387177988887/0.5826480388641357 \n",
      "\tG: 0.8284717202186584 (Real: [3.9170712402462957, 1.1958505472295351], Fake: [-0.71227937549352649, 0.2911438312114859]\n",
      "\n",
      ") \n",
      "1800: \n",
      "\tD: 0.000559665379114449/0.5582872629165649 \n",
      "\tG: 0.9756823778152466 (Real: [3.9077332699298859, 1.2453006594722849], Fake: [-0.20520408965647222, 0.39624735380061182]\n",
      "\n",
      ") \n",
      "2000: \n",
      "\tD: 0.007776497397571802/0.7506114840507507 \n",
      "\tG: 0.5568863749504089 (Real: [4.1208323407173157, 1.0467637994254031], Fake: [0.56084613159298902, 0.46919559345989664]\n",
      "\n",
      ") \n",
      "2200: \n",
      "\tD: 0.1373027116060257/0.4732701778411865 \n",
      "\tG: 0.9948362112045288 (Real: [3.857092098593712, 1.2605003090301541], Fake: [0.74420135661959652, 0.5373348877446561]\n",
      "\n",
      ") \n",
      "2400: \n",
      "\tD: 0.036504920572042465/0.4548376202583313 \n",
      "\tG: 0.9854828715324402 (Real: [3.6823674857616426, 1.2583640721638198], Fake: [0.97075652360916143, 0.61721827389703232]\n",
      "\n",
      ") \n",
      "2600: \n",
      "\tD: 0.12143528461456299/0.367430180311203 \n",
      "\tG: 1.1604697704315186 (Real: [4.1340910064987835, 1.3610366806673035], Fake: [1.1948604802787304, 0.62486812851428686]\n",
      "\n",
      ") \n",
      "2800: \n",
      "\tD: 0.30438482761383057/0.3648008406162262 \n",
      "\tG: 1.1760085821151733 (Real: [4.1288285422325135, 1.185322691480422], Fake: [1.5219533768296243, 0.82114105792984471]\n",
      "\n",
      ") \n",
      "3000: \n",
      "\tD: 0.20828154683113098/0.40906521677970886 \n",
      "\tG: 0.947483479976654 (Real: [3.8279611569643022, 1.2064474087080346], Fake: [1.8808494463562966, 0.91260691908422198]\n",
      "\n",
      ") \n",
      "3200: \n",
      "\tD: 0.209053635597229/0.6929312944412231 \n",
      "\tG: 1.1014186143875122 (Real: [3.9530583989620207, 1.3293850741646251], Fake: [2.3326074326038362, 1.0262087850906694]\n",
      "\n",
      ") \n",
      "3400: \n",
      "\tD: 0.5677819848060608/0.5360791087150574 \n",
      "\tG: 0.857850193977356 (Real: [3.8682297279220075, 1.2235121956346708], Fake: [2.6560236054658888, 1.0008544129529127]\n",
      "\n",
      ") \n",
      "3600: \n",
      "\tD: 0.28774121403694153/0.5312647223472595 \n",
      "\tG: 0.9530551433563232 (Real: [4.0957919478416445, 1.2606793434389871], Fake: [3.179175878763199, 1.1097289385791349]\n",
      "\n",
      ") \n",
      "3800: \n",
      "\tD: 0.6302505731582642/0.6956143975257874 \n",
      "\tG: 0.5823072195053101 (Real: [3.8209987336397173, 1.1770908527013033], Fake: [3.4789903247356415, 1.138830440605026]\n",
      "\n",
      ") \n",
      "4000: \n",
      "\tD: 0.7305523157119751/0.7175982594490051 \n",
      "\tG: 0.732933521270752 (Real: [3.9888023769855501, 1.1650514174849294], Fake: [3.803028482198715, 1.206458656671074]\n",
      "\n",
      ") \n",
      "4200: \n",
      "\tD: 0.6416077613830566/0.7691584825515747 \n",
      "\tG: 0.5841974020004272 (Real: [3.8604411506652831, 1.2374602896339904], Fake: [4.1144592869281773, 1.2234943125018112]\n",
      "\n",
      ") \n",
      "4400: \n",
      "\tD: 1.0532119274139404/0.5782573819160461 \n",
      "\tG: 0.531866192817688 (Real: [3.8895836472511292, 1.1535335990047508], Fake: [4.2610238647460941, 1.2519408156698124]\n",
      "\n",
      ") \n",
      "4600: \n",
      "\tD: 0.9558539986610413/0.6637031435966492 \n",
      "\tG: 0.7249554395675659 (Real: [3.9870082557201387, 1.3179713883287252], Fake: [4.4285840260982514, 1.4552032153420764]\n",
      "\n",
      ") \n",
      "4800: \n",
      "\tD: 0.7879146933555603/1.257649302482605 \n",
      "\tG: 0.7571307420730591 (Real: [3.9796071028709412, 1.3902369009479172], Fake: [4.9471641445159911, 1.2362218066540369]\n",
      "\n",
      ") \n",
      "5000: \n",
      "\tD: 1.0809261798858643/0.7941840887069702 \n",
      "\tG: 0.583426296710968 (Real: [3.572335907071829, 1.1402129153022513], Fake: [5.0702262282371517, 1.3383475192062926]\n",
      "\n",
      ") \n",
      "5200: \n",
      "\tD: 0.8249039053916931/0.866692066192627 \n",
      "\tG: 0.7398788928985596 (Real: [4.0091104829311375, 1.1532881259036869], Fake: [5.0160799145698549, 1.3540550573030592]\n",
      "\n",
      ") \n",
      "5400: \n",
      "\tD: 1.3866026401519775/0.6130801439285278 \n",
      "\tG: 0.9699705243110657 (Real: [4.0147054132819173, 1.1986214927937391], Fake: [5.186288678646088, 1.4097393180681186]\n",
      "\n",
      ") \n",
      "5600: \n",
      "\tD: 0.9082789421081543/0.4820172190666199 \n",
      "\tG: 0.6141557693481445 (Real: [3.8825532687455415, 1.1932775034239471], Fake: [5.1259259319305421, 1.5329688803676811]\n",
      "\n",
      ") \n",
      "5800: \n",
      "\tD: 1.312857747077942/0.6849784255027771 \n",
      "\tG: 0.4519132077693939 (Real: [4.0164412319660183, 1.0963431903379892], Fake: [5.2734743165969853, 1.5473270783528685]\n",
      "\n",
      ") \n",
      "6000: \n",
      "\tD: 0.8438501358032227/0.7155312299728394 \n",
      "\tG: 0.5134735703468323 (Real: [4.2660441386699679, 1.202623769324608], Fake: [5.3882702660560611, 1.4919546163132695]\n",
      "\n",
      ") \n",
      "6200: \n",
      "\tD: 1.1331568956375122/0.866873025894165 \n",
      "\tG: 0.8785350918769836 (Real: [4.0381707674264904, 1.1101560561451471], Fake: [5.6347030901908877, 1.4087548248892892]\n",
      "\n",
      ") \n",
      "6400: \n",
      "\tD: 1.0676429271697998/0.6291508674621582 \n",
      "\tG: 0.5629856586456299 (Real: [3.9781798183918, 1.2046675965442668], Fake: [5.8205167484283447, 1.4839111402352221]\n",
      "\n",
      ") \n",
      "6600: \n",
      "\tD: 1.085699200630188/0.41251224279403687 \n",
      "\tG: 0.8190202713012695 (Real: [4.1657136392593381, 1.2382600167127875], Fake: [5.610710642337799, 1.5325330641085173]\n",
      "\n",
      ") \n",
      "6800: \n",
      "\tD: 0.9887478351593018/0.2722961902618408 \n",
      "\tG: 1.1919100284576416 (Real: [3.9057185292243957, 1.2746255649383158], Fake: [5.8044539332389835, 1.5023241803817866]\n",
      "\n",
      ") \n",
      "7000: \n",
      "\tD: 0.7662496566772461/0.7199170589447021 \n",
      "\tG: 1.00117826461792 (Real: [4.1627907776832584, 1.1822531296726699], Fake: [5.6787666034698487, 1.5644025424931391]\n",
      "\n",
      ") \n",
      "7200: \n",
      "\tD: 0.9995802640914917/0.5290235877037048 \n",
      "\tG: 1.0297951698303223 (Real: [3.9344382733106613, 1.3504993352070704], Fake: [5.660096907615662, 1.5390666009979383]\n",
      "\n",
      ") \n",
      "7400: \n",
      "\tD: 0.8264977335929871/0.6100553870201111 \n",
      "\tG: 1.2207516431808472 (Real: [3.941649835705757, 1.1939322818491811], Fake: [5.7030301868915556, 1.5603992502133908]\n",
      "\n",
      ") \n",
      "7600: \n",
      "\tD: 0.7145795822143555/0.6968717575073242 \n",
      "\tG: 0.7719062566757202 (Real: [4.041872819215059, 1.2116982293025591], Fake: [5.7984921622276309, 1.2949397170407295]\n",
      "\n",
      ") \n",
      "7800: \n",
      "\tD: 0.6475167870521545/0.5723186135292053 \n",
      "\tG: 0.8573028445243835 (Real: [3.8832768827676771, 1.2633540502907687], Fake: [5.3547982478141787, 1.4485444441788624]\n",
      "\n",
      ") \n",
      "8000: \n",
      "\tD: 0.9131264686584473/0.4709224998950958 \n",
      "\tG: 1.038899540901184 (Real: [4.0748598939180374, 1.1667005871156957], Fake: [5.4126900219917298, 1.3431713020183045]\n",
      "\n",
      ") \n",
      "8200: \n",
      "\tD: 0.6690130233764648/0.5437739491462708 \n",
      "\tG: 0.9283447861671448 (Real: [3.7645450984407218, 1.3769717471659055], Fake: [5.1951387000083926, 1.4441109836787329]\n",
      "\n",
      ") \n",
      "8400: \n",
      "\tD: 0.6154206991195679/0.6022387146949768 \n",
      "\tG: 0.7715932726860046 (Real: [4.0711683833599093, 1.1406674031588255], Fake: [4.9646431863307949, 1.4528017839090455]\n",
      "\n",
      ") \n",
      "8600: \n",
      "\tD: 0.7473695874214172/0.47201934456825256 \n",
      "\tG: 0.8630300164222717 (Real: [4.0121331775188445, 1.1684204935152642], Fake: [4.711700826883316, 1.4542582242857249]\n",
      "\n",
      ") \n",
      "8800: \n",
      "\tD: 0.6429199576377869/0.5585044622421265 \n",
      "\tG: 0.7679177522659302 (Real: [4.0811715710163119, 1.1625947456031969], Fake: [4.2485862398147587, 1.5754269807818462]\n",
      "\n",
      ") \n",
      "9000: \n",
      "\tD: 0.7664341926574707/0.5236667990684509 \n",
      "\tG: 0.7163258790969849 (Real: [3.9494499129056932, 1.1331052848496723], Fake: [4.2930837857723239, 1.3623902040067148]\n",
      "\n",
      ") \n",
      "9200: \n",
      "\tD: 0.7784073948860168/0.513613224029541 \n",
      "\tG: 0.713394820690155 (Real: [3.8117551815509798, 1.332111328638564], Fake: [3.9973025929927828, 1.2948375242426737]\n",
      "\n",
      ") \n",
      "9400: \n",
      "\tD: 0.6448866128921509/0.6591049432754517 \n",
      "\tG: 0.5427834987640381 (Real: [4.009708521366119, 1.233760354564795], Fake: [3.9373957061767579, 1.2358940110659988]\n",
      "\n",
      ") \n",
      "9600: \n",
      "\tD: 0.7562643885612488/0.7281373143196106 \n",
      "\tG: 0.8101569414138794 (Real: [4.2096995067596437, 1.0595389808352691], Fake: [4.0015936493873596, 1.0779502199553446]\n",
      "\n",
      ") \n",
      "9800: \n",
      "\tD: 0.5669987201690674/0.7370203137397766 \n",
      "\tG: 0.5974071621894836 (Real: [4.0519598245620729, 1.0741282818828135], Fake: [3.5990794324874877, 1.0794943415554661]\n",
      "\n",
      ") \n",
      "10000: \n",
      "\tD: 0.7622048854827881/0.7960547804832458 \n",
      "\tG: 0.6628720164299011 (Real: [3.8250626954436302, 1.3099549910509412], Fake: [3.282262789607048, 1.1583053259384481]\n",
      "\n",
      ") \n",
      "10200: \n",
      "\tD: 0.6219887733459473/0.671296238899231 \n",
      "\tG: 0.6045224070549011 (Real: [3.6410240995883942, 1.030544142120619], Fake: [3.0492695945501329, 1.1703835972432164]\n",
      "\n",
      ") \n",
      "10400: \n",
      "\tD: 0.5360044240951538/0.679516077041626 \n",
      "\tG: 0.6980401277542114 (Real: [4.1484576237201694, 1.3391232952647194], Fake: [3.224893154501915, 1.1394885086685322]\n",
      "\n",
      ") \n",
      "10600: \n",
      "\tD: 0.636553168296814/0.6997240781784058 \n",
      "\tG: 0.5955265760421753 (Real: [4.0754688930511476, 1.1432442045149196], Fake: [3.2392380869388582, 1.0934259716479051]\n",
      "\n",
      ") \n",
      "10800: \n",
      "\tD: 0.4714139699935913/0.6854062080383301 \n",
      "\tG: 0.7138891220092773 (Real: [3.8621654492616653, 1.3853748847863609], Fake: [3.4105654013156892, 1.0222259304796175]\n",
      "\n",
      ") \n",
      "11000: \n",
      "\tD: 0.503561794757843/0.6385814547538757 \n",
      "\tG: 0.7197462916374207 (Real: [3.9757840235531332, 1.2619720204094849], Fake: [3.7577773725986479, 1.0964531327510691]\n",
      "\n",
      ") \n",
      "11200: \n",
      "\tD: 0.6238171458244324/0.6942808032035828 \n",
      "\tG: 0.65158611536026 (Real: [4.2701029872894285, 1.2850072711679745], Fake: [3.9389949131011961, 1.1997031185860425]\n",
      "\n",
      ") \n",
      "11400: \n",
      "\tD: 0.6667223572731018/0.7565324306488037 \n",
      "\tG: 0.6253471970558167 (Real: [3.9536984503269195, 1.0713160215675008], Fake: [3.7755676925182344, 1.5402243979452075]\n",
      "\n",
      ") \n",
      "11600: \n",
      "\tD: 0.5471203923225403/0.7626245617866516 \n",
      "\tG: 0.5951851606369019 (Real: [4.0573672485351562, 1.2642169741636291], Fake: [4.2718993705511092, 1.395962487293988]\n",
      "\n",
      ") \n",
      "11800: \n",
      "\tD: 0.838039219379425/0.5319454669952393 \n",
      "\tG: 0.6212660670280457 (Real: [3.8678278738260268, 1.2633453044989305], Fake: [4.4142443132400508, 1.3729305893714445]\n",
      "\n",
      ") \n",
      "12000: \n",
      "\tD: 0.7924734354019165/0.689354658126831 \n",
      "\tG: 0.8096297383308411 (Real: [4.15865260720253, 1.156296749914413], Fake: [4.8686832356452943, 1.1516770915155465]\n",
      "\n",
      ") \n",
      "12200: \n",
      "\tD: 0.7945328950881958/0.661250650882721 \n",
      "\tG: 0.7427548170089722 (Real: [3.8517259192466735, 1.1894282084804229], Fake: [4.7063837671279911, 1.200269509342264]\n",
      "\n",
      ") \n",
      "12400: \n",
      "\tD: 0.8768939971923828/0.596903383731842 \n",
      "\tG: 0.8807814717292786 (Real: [3.7579427397251131, 1.3316279590887541], Fake: [4.7679125714302062, 1.2685051514235826]\n",
      "\n",
      ") \n",
      "12600: \n",
      "\tD: 1.0261601209640503/0.8066633939743042 \n",
      "\tG: 0.7062594890594482 (Real: [3.9482839384675028, 1.2946604693827037], Fake: [3.9574313938617705, 1.5098443347050654]\n",
      "\n",
      ") \n",
      "12800: \n",
      "\tD: 0.7274628281593323/0.7156655788421631 \n",
      "\tG: 0.6620020866394043 (Real: [3.9231190180778501, 1.0570250873753853], Fake: [3.9470200234651567, 1.5262095104320388]\n",
      "\n",
      ") \n",
      "13000: \n",
      "\tD: 0.5754385590553284/0.5824340581893921 \n",
      "\tG: 0.6493961811065674 (Real: [4.2025528150796889, 1.2346491683487266], Fake: [3.5701048851013182, 1.4740203899961049]\n",
      "\n",
      ") \n",
      "13200: \n",
      "\tD: 0.8555935025215149/0.7253465056419373 \n",
      "\tG: 0.8416574597358704 (Real: [4.0901651418209077, 1.2990624980715024], Fake: [4.2138187754154206, 1.0962501357980166]\n",
      "\n",
      ") \n",
      "13400: \n",
      "\tD: 0.6043583750724792/0.769635796546936 \n",
      "\tG: 0.6322720646858215 (Real: [4.0718252742290497, 1.2017051894544395], Fake: [4.1058875370025634, 1.1061745230266855]\n",
      "\n",
      ") \n",
      "13600: \n",
      "\tD: 0.7092161774635315/0.784078061580658 \n",
      "\tG: 0.6124345064163208 (Real: [4.0443200731277464, 1.2782512349704498], Fake: [4.012702207565308, 1.0542344423620065]\n",
      "\n",
      ") \n",
      "13800: \n",
      "\tD: 0.6462637186050415/0.6894661784172058 \n",
      "\tG: 0.7230322360992432 (Real: [3.9343971419334411, 1.2443316540762839], Fake: [4.0667479968070985, 1.0565318798532324]\n",
      "\n",
      ") \n",
      "14000: \n",
      "\tD: 0.6488564610481262/0.7058229446411133 \n",
      "\tG: 0.8202340602874756 (Real: [3.9890145587921144, 1.1694959315774791], Fake: [3.758971234560013, 1.1112815464794594]\n",
      "\n",
      ") \n",
      "14200: \n",
      "\tD: 0.5524952411651611/0.8272920846939087 \n",
      "\tG: 0.6404784917831421 (Real: [4.1373578822612762, 1.3351001584980524], Fake: [3.2942091816663743, 1.2957554667369182]\n",
      "\n",
      ") \n",
      "14400: \n",
      "\tD: 0.5812056064605713/0.661388099193573 \n",
      "\tG: 0.5932680368423462 (Real: [4.0090053939819335, 1.3041268063770513], Fake: [3.2920059502124785, 1.4643276408392221]\n",
      "\n",
      ") \n",
      "14600: \n",
      "\tD: 0.7328266501426697/0.7843930125236511 \n",
      "\tG: 0.588738203048706 (Real: [4.0742049324512486, 1.2077273039856049], Fake: [3.9706184232234953, 1.2689287378129614]\n",
      "\n",
      ") \n",
      "14800: \n",
      "\tD: 0.6827071309089661/0.6603748798370361 \n",
      "\tG: 0.7792729139328003 (Real: [3.7657661485671996, 1.2560625694075758], Fake: [4.1993233609199523, 1.1946929691569874]\n",
      "\n",
      ") \n",
      "15000: \n",
      "\tD: 0.5426215529441833/0.6385683417320251 \n",
      "\tG: 0.6369542479515076 (Real: [4.1158827602863308, 1.2972396967304982], Fake: [4.6567915558815001, 1.0266254907964878]\n",
      "\n",
      ") \n",
      "15200: \n",
      "\tD: 0.6389331221580505/0.7211231589317322 \n",
      "\tG: 0.8240220546722412 (Real: [3.9547055417299273, 1.3142176335088656], Fake: [4.3087378358840942, 1.1257287859961698]\n",
      "\n",
      ") \n",
      "15400: \n",
      "\tD: 0.6045348644256592/0.6098337173461914 \n",
      "\tG: 0.6919584274291992 (Real: [4.1670655059814452, 1.3299705781859368], Fake: [4.0631486558914185, 1.2637645687665635]\n",
      "\n",
      ") \n",
      "15600: \n",
      "\tD: 0.7242277264595032/0.591827392578125 \n",
      "\tG: 0.6285288333892822 (Real: [4.1370382988452912, 1.2949278830248274], Fake: [3.7039457947015761, 1.4110380184996947]\n",
      "\n",
      ") \n",
      "15800: \n",
      "\tD: 0.6150974631309509/0.7339227199554443 \n",
      "\tG: 0.6922637820243835 (Real: [4.0707716572284696, 1.3200937359954235], Fake: [3.6496827518939972, 1.2959724679026523]\n",
      "\n",
      ") \n",
      "16000: \n",
      "\tD: 0.8173749446868896/0.6958287954330444 \n",
      "\tG: 0.8417837619781494 (Real: [3.9367883628606797, 1.2405890044538179], Fake: [3.9269669318199156, 1.3090446059316485]\n",
      "\n",
      ") \n",
      "16200: \n",
      "\tD: 0.7364622950553894/0.6346436142921448 \n",
      "\tG: 0.692354142665863 (Real: [3.8859644210338593, 1.0859912182706659], Fake: [4.1362739372253419, 1.1239055072322559]\n",
      "\n",
      ") \n",
      "16400: \n",
      "\tD: 0.7260401248931885/0.6867427229881287 \n",
      "\tG: 0.662729799747467 (Real: [4.1798612260818482, 1.3213928193836182], Fake: [4.3085159373283384, 1.0437808405443694]\n",
      "\n",
      ") \n",
      "16600: \n",
      "\tD: 0.7064386010169983/0.7648332118988037 \n",
      "\tG: 0.6877800226211548 (Real: [4.210918045043945, 1.0536274641463996], Fake: [4.0916322934627534, 1.1745235647477574]\n",
      "\n",
      ") \n",
      "16800: \n",
      "\tD: 0.49783575534820557/0.6605909466743469 \n",
      "\tG: 0.744341254234314 (Real: [3.9110048460960387, 1.252480296911296], Fake: [4.0495660293102267, 1.1050820279823195]\n",
      "\n",
      ") \n",
      "17000: \n",
      "\tD: 0.5770854353904724/0.6382680535316467 \n",
      "\tG: 0.7550545930862427 (Real: [3.9141266703605653, 1.2436032433477544], Fake: [3.6472703635692598, 1.3101960047284669]\n",
      "\n",
      ") \n",
      "17200: \n",
      "\tD: 0.5067523717880249/0.7579411864280701 \n",
      "\tG: 0.7029204368591309 (Real: [3.9058580613136291, 1.3727742608639835], Fake: [3.5359730565547944, 1.4214486617313944]\n",
      "\n",
      ") \n",
      "17400: \n",
      "\tD: 0.8563867807388306/0.6559692621231079 \n",
      "\tG: 0.7640907764434814 (Real: [3.8404009079933168, 1.3052589598239415], Fake: [3.710836960673332, 1.3514801459965633]\n",
      "\n",
      ") \n",
      "17600: \n",
      "\tD: 0.6502057313919067/0.7789579629898071 \n",
      "\tG: 0.6027262806892395 (Real: [3.8459307365119457, 1.2855450420793779], Fake: [3.9181269800662992, 1.2004939710326623]\n",
      "\n",
      ") \n",
      "17800: \n",
      "\tD: 0.6382248997688293/0.7323664426803589 \n",
      "\tG: 0.7044427394866943 (Real: [4.024252260327339, 1.1353267402716656], Fake: [4.086213804483414, 1.1453163550858798]\n",
      "\n",
      ") \n",
      "18000: \n",
      "\tD: 0.6113450527191162/0.6156586408615112 \n",
      "\tG: 0.7441794276237488 (Real: [3.9479484701156617, 1.3557731673880971], Fake: [4.3595293867588047, 1.011863245876705]\n",
      "\n",
      ") \n",
      "18200: \n",
      "\tD: 0.8141829371452332/0.7918493151664734 \n",
      "\tG: 0.7609230875968933 (Real: [3.9603831958770752, 1.2612175074917173], Fake: [4.0827543771266939, 1.2444119685527348]\n",
      "\n",
      ") \n",
      "18400: \n",
      "\tD: 0.7511516809463501/0.758566677570343 \n",
      "\tG: 0.8918177485466003 (Real: [4.1435691261291501, 1.0970728795903621], Fake: [3.6538132399320604, 1.4814304676833154]\n",
      "\n",
      ") \n",
      "18600: \n",
      "\tD: 0.514473557472229/0.5357884764671326 \n",
      "\tG: 0.8104627132415771 (Real: [4.2543726444244383, 1.0840621818182505], Fake: [3.5764578330516814, 1.4174540026266484]\n",
      "\n",
      ") \n",
      "18800: \n",
      "\tD: 0.6629346013069153/0.6318196654319763 \n",
      "\tG: 0.7197077870368958 (Real: [3.8842361283302309, 1.2500219316417716], Fake: [3.9303382933139801, 1.2718451033805942]\n",
      "\n",
      ") \n",
      "19000: \n",
      "\tD: 0.7225376963615417/0.8822591304779053 \n",
      "\tG: 0.6499986052513123 (Real: [3.9501834261417388, 1.2640556728250092], Fake: [4.3339909493923185, 1.041208591052678]\n",
      "\n",
      ") \n",
      "19200: \n",
      "\tD: 0.6748338341712952/0.5908384919166565 \n",
      "\tG: 0.6960151791572571 (Real: [4.0466001355648045, 1.4760373405520852], Fake: [4.256101528406143, 1.110838024740449]\n",
      "\n",
      ") \n",
      "19400: \n",
      "\tD: 0.5553245544433594/0.7964349389076233 \n",
      "\tG: 0.6045649647712708 (Real: [4.1772722077369693, 1.3881676160113225], Fake: [4.1691428017616268, 1.1835199504956777]\n",
      "\n",
      ") \n",
      "19600: \n",
      "\tD: 0.6316629648208618/1.0094984769821167 \n",
      "\tG: 0.5076994299888611 (Real: [4.2849174427986148, 1.1597543043331133], Fake: [3.8329500460624697, 1.4187788356276199]\n",
      "\n",
      ") \n",
      "19800: \n",
      "\tD: 0.7140761613845825/0.7478976845741272 \n",
      "\tG: 0.7779703140258789 (Real: [3.9254047989845278, 1.1520112952181558], Fake: [3.6980342996120452, 1.4063571603615383]\n",
      "\n",
      ") \n",
      "20000: \n",
      "\tD: 0.7227519750595093/0.6550763845443726 \n",
      "\tG: 0.6814348101615906 (Real: [4.084558181762695, 1.336642300706451], Fake: [3.6412225759029386, 1.4215839494034936]\n",
      "\n",
      ") \n",
      "20200: \n",
      "\tD: 0.8496253490447998/0.6312081217765808 \n",
      "\tG: 0.7709009647369385 (Real: [4.0099958163499831, 1.3785853440398679], Fake: [3.9318144851922989, 1.2815857428400903]\n",
      "\n",
      ") \n",
      "20400: \n",
      "\tD: 0.8605225682258606/0.7413907051086426 \n",
      "\tG: 0.8570367097854614 (Real: [4.0391417694091798, 1.248437215522344], Fake: [4.1759705305099484, 0.97383058968982572]\n",
      "\n",
      ") \n",
      "20600: \n",
      "\tD: 0.7206630706787109/0.7135744690895081 \n",
      "\tG: 0.6936419010162354 (Real: [4.0010190820693969, 1.2420381888162031], Fake: [4.063850229978561, 1.1464662746065757]\n",
      "\n",
      ") \n",
      "20800: \n",
      "\tD: 0.6574440598487854/0.6535695791244507 \n",
      "\tG: 0.7014876008033752 (Real: [3.9849442839622498, 1.2130652893163889], Fake: [4.2453355932235715, 0.99347045835609105]\n",
      "\n",
      ") \n",
      "21000: \n",
      "\tD: 0.6492858529090881/0.8224915862083435 \n",
      "\tG: 0.7830770015716553 (Real: [4.2390534043312069, 1.2847317542059096], Fake: [3.4256736516952513, 1.3937795548639293]\n",
      "\n",
      ") \n",
      "21200: \n",
      "\tD: 0.7248936295509338/0.6850764751434326 \n",
      "\tG: 0.6292489767074585 (Real: [3.9391552996635437, 1.1459630795839646], Fake: [3.7619462952017786, 1.3395350875065564]\n",
      "\n",
      ") \n",
      "21400: \n",
      "\tD: 0.48871076107025146/1.0048768520355225 \n",
      "\tG: 0.7776311039924622 (Real: [4.0705947959423066, 1.2609248147581846], Fake: [3.752903448343277, 1.3611821191397346]\n",
      "\n",
      ") \n",
      "21600: \n",
      "\tD: 0.7193803787231445/0.6805363297462463 \n",
      "\tG: 0.8330891132354736 (Real: [3.8477943497896194, 1.2573655230994467], Fake: [4.0124573582410816, 1.1742983495885155]\n",
      "\n",
      ") \n",
      "21800: \n",
      "\tD: 0.6746833920478821/0.555318295955658 \n",
      "\tG: 0.7356593608856201 (Real: [4.0926191520690915, 1.2705111479936029], Fake: [4.0610429620742794, 1.1717496953337061]\n",
      "\n",
      ") \n",
      "22000: \n",
      "\tD: 0.811349093914032/0.693854570388794 \n",
      "\tG: 0.8562057614326477 (Real: [3.9636466097831726, 1.1879402868277793], Fake: [4.0691471672058102, 1.2089370241920874]\n",
      "\n",
      ") \n",
      "22200: \n",
      "\tD: 0.6649767160415649/0.6157183051109314 \n",
      "\tG: 0.6903569102287292 (Real: [3.9636031502485274, 1.2160479952519643], Fake: [3.9498189187049864, 1.2408227442657143]\n",
      "\n",
      ") \n",
      "22400: \n",
      "\tD: 0.8607138991355896/0.6970067620277405 \n",
      "\tG: 0.7332705855369568 (Real: [4.0596135866641996, 1.2218337882050809], Fake: [3.7680002111196518, 1.2667271302330987]\n",
      "\n",
      ") \n",
      "22600: \n",
      "\tD: 0.5725809335708618/0.6936918497085571 \n",
      "\tG: 0.9364858865737915 (Real: [4.0909672784805302, 1.2510586080084853], Fake: [3.8322920382022856, 1.2652718871914586]\n",
      "\n",
      ") \n",
      "22800: \n",
      "\tD: 0.7353342175483704/0.9475412368774414 \n",
      "\tG: 0.6128917336463928 (Real: [3.9107275626063345, 1.4230429955874389], Fake: [4.0001196789741513, 1.1767897522927648]\n",
      "\n",
      ") \n",
      "23000: \n",
      "\tD: 0.8660377264022827/0.6299121379852295 \n",
      "\tG: 0.5574884414672852 (Real: [3.9167625579237937, 1.2697785135882933], Fake: [4.0763805198669436, 1.1797125767861476]\n",
      "\n",
      ") \n",
      "23200: \n",
      "\tD: 0.720511794090271/0.7341483235359192 \n",
      "\tG: 0.6908716559410095 (Real: [3.9835308110713958, 1.0751768565336313], Fake: [4.1242146158218382, 1.2797602624934266]\n",
      "\n",
      ") \n",
      "23400: \n",
      "\tD: 0.6460877656936646/0.617578387260437 \n",
      "\tG: 0.769388735294342 (Real: [4.1105763183534147, 1.3369122916369123], Fake: [4.1088344508409502, 1.2669963480156983]\n",
      "\n",
      ") \n",
      "23600: \n",
      "\tD: 0.7816430330276489/0.6097612977027893 \n",
      "\tG: 0.68914395570755 (Real: [3.8793751657009126, 1.1159115564510829], Fake: [3.7620958551764487, 1.4505912886753425]\n",
      "\n",
      ") \n",
      "23800: \n",
      "\tD: 0.624545156955719/0.6364009380340576 \n",
      "\tG: 0.5489101409912109 (Real: [4.0078776443004607, 1.231435654382818], Fake: [3.7254796981811524, 1.501500168081068]\n",
      "\n",
      ") \n",
      "24000: \n",
      "\tD: 0.6720515489578247/0.833474338054657 \n",
      "\tG: 0.6226930022239685 (Real: [3.953004577755928, 1.1510677636561391], Fake: [4.091116546392441, 1.1700682448599669]\n",
      "\n",
      ") \n",
      "24200: \n",
      "\tD: 0.5970328450202942/0.6463072299957275 \n",
      "\tG: 0.7746307253837585 (Real: [4.0536308920383455, 1.1380927798450926], Fake: [4.2474910843372342, 1.0180703339981692]\n",
      "\n",
      ") \n",
      "24400: \n",
      "\tD: 0.7289245128631592/0.7810133695602417 \n",
      "\tG: 0.6941294074058533 (Real: [3.7311806797981264, 1.2990646831523611], Fake: [4.0859742003679278, 1.3486380166431695]\n",
      "\n",
      ") \n",
      "24600: \n",
      "\tD: 0.6227313280105591/0.7098525762557983 \n",
      "\tG: 0.5553202629089355 (Real: [3.8027708351612093, 1.2292381308146996], Fake: [4.0403966701030729, 1.3305993650847554]\n",
      "\n",
      ") \n",
      "24800: \n",
      "\tD: 0.722968339920044/0.6285145878791809 \n",
      "\tG: 0.9056298732757568 (Real: [4.0445358085632321, 1.2500358013126698], Fake: [4.0403956949710844, 1.2619282571685049]\n",
      "\n",
      ") \n",
      "25000: \n",
      "\tD: 0.7080971598625183/0.9006895422935486 \n",
      "\tG: 0.665664553642273 (Real: [4.0089350122213361, 1.2647385256970927], Fake: [3.9750086277723313, 1.2705177222999893]\n",
      "\n",
      ") \n",
      "25200: \n",
      "\tD: 0.7519137263298035/0.7827786803245544 \n",
      "\tG: 0.7653723955154419 (Real: [4.1206322395801545, 1.1633536382819347], Fake: [3.9963835108280183, 1.2393257368557418]\n",
      "\n",
      ") \n",
      "25400: \n",
      "\tD: 0.49639442563056946/0.5477220416069031 \n",
      "\tG: 0.713599443435669 (Real: [3.9349768459796906, 1.2726728033320116], Fake: [3.8996744227409361, 1.2286885194609363]\n",
      "\n",
      ") \n",
      "25600: \n",
      "\tD: 0.8181589841842651/0.7409186363220215 \n",
      "\tG: 0.7184632420539856 (Real: [4.1113450157642362, 1.2680889849561099], Fake: [3.942342920899391, 1.2038638654440184]\n",
      "\n",
      ") \n",
      "25800: \n",
      "\tD: 0.7703770399093628/0.6182390451431274 \n",
      "\tG: 0.8799975514411926 (Real: [3.9095029020309449, 1.14493284077215], Fake: [3.9716505742073061, 1.3250540534067317]\n",
      "\n",
      ") \n",
      "26000: \n",
      "\tD: 0.6425869464874268/0.5051880478858948 \n",
      "\tG: 0.8443096280097961 (Real: [4.2447081136703488, 1.2776917831714585], Fake: [3.5976223790645601, 1.4489408187074109]\n",
      "\n",
      ") \n",
      "26200: \n",
      "\tD: 0.5552046895027161/0.6361969113349915 \n",
      "\tG: 0.700086236000061 (Real: [3.8264694795757532, 1.1704831768344937], Fake: [4.0071568465232845, 1.2246727618018096]\n",
      "\n",
      ") \n",
      "26400: \n",
      "\tD: 0.8025238513946533/0.6393616199493408 \n",
      "\tG: 0.5598083138465881 (Real: [4.0966656804084778, 1.1192281396650825], Fake: [3.9591621625423432, 1.3034881566908809]\n",
      "\n",
      ") \n",
      "26600: \n",
      "\tD: 0.6606972813606262/0.7777670621871948 \n",
      "\tG: 0.7105637192726135 (Real: [4.1407259875535969, 1.1558818538394526], Fake: [4.0829351729154588, 1.1856095238558064]\n",
      "\n",
      ") \n",
      "26800: \n",
      "\tD: 0.7294872999191284/0.7115178108215332 \n",
      "\tG: 0.7023298144340515 (Real: [4.068820324391127, 1.2711211626356593], Fake: [3.8042196375131607, 1.4318730413897207]\n",
      "\n",
      ") \n",
      "27000: \n",
      "\tD: 0.8893887996673584/0.5248200297355652 \n",
      "\tG: 0.7812089920043945 (Real: [4.0868408894538879, 1.1982878568259361], Fake: [3.8780226171016694, 1.3904791936314678]\n",
      "\n",
      ") \n",
      "27200: \n",
      "\tD: 0.7239569425582886/0.7536263465881348 \n",
      "\tG: 0.5607108473777771 (Real: [3.9255185896158218, 1.3078198066268867], Fake: [4.0240087205171582, 1.2377801384239637]\n",
      "\n",
      ") \n",
      "27400: \n",
      "\tD: 0.6695931553840637/0.6330161690711975 \n",
      "\tG: 0.6846320629119873 (Real: [4.0335773801803585, 1.1466683931183279], Fake: [4.0560519403219226, 1.3127517756229534]\n",
      "\n",
      ") \n",
      "27600: \n",
      "\tD: 0.6520873308181763/0.7363713383674622 \n",
      "\tG: 0.5406187176704407 (Real: [3.8646525645256045, 1.307161156330549], Fake: [3.962492115497589, 1.4546286944487152]\n",
      "\n",
      ") \n",
      "27800: \n",
      "\tD: 0.706864595413208/0.5997506380081177 \n",
      "\tG: 0.7462658882141113 (Real: [4.1112540435791018, 1.2053823496977947], Fake: [4.0926586139202117, 1.2998135195217386]\n",
      "\n",
      ") \n",
      "28000: \n",
      "\tD: 0.7577580213546753/0.6859257817268372 \n",
      "\tG: 0.7491325736045837 (Real: [4.0682484304904936, 1.091638271528012], Fake: [4.1463345825672153, 1.2795950839004593]\n",
      "\n",
      ") \n",
      "28200: \n",
      "\tD: 0.6869932413101196/0.6621490716934204 \n",
      "\tG: 0.759099543094635 (Real: [3.7293970751762391, 1.2698292413250676], Fake: [4.2820810770988462, 1.089097794787965]\n",
      "\n",
      ") \n",
      "28400: \n",
      "\tD: 0.8286072611808777/0.6634415984153748 \n",
      "\tG: 0.7634910941123962 (Real: [4.0424593234062192, 1.0682838832849229], Fake: [4.1643523079156877, 1.2635765254478992]\n",
      "\n",
      ") \n",
      "28600: \n",
      "\tD: 0.7939627766609192/0.6787195801734924 \n",
      "\tG: 0.5900062918663025 (Real: [3.8932848072052, 1.2671161120842389], Fake: [3.8894446980953217, 1.3551467501021459]\n",
      "\n",
      ") \n",
      "28800: \n",
      "\tD: 0.6772173643112183/0.7338019013404846 \n",
      "\tG: 0.8513179421424866 (Real: [4.0121565437316891, 1.1360128128662208], Fake: [4.0355883401632306, 1.2533020751104436]\n",
      "\n",
      ") \n",
      "29000: \n",
      "\tD: 0.6002867221832275/0.6396306157112122 \n",
      "\tG: 0.7824130654335022 (Real: [3.8233799409866331, 1.3640658336206521], Fake: [3.891934769153595, 1.4405748213066638]\n",
      "\n",
      ") \n",
      "29200: \n",
      "\tD: 0.6999064087867737/0.6978997588157654 \n",
      "\tG: 0.6686890721321106 (Real: [3.9928779757022856, 1.1970952063658045], Fake: [4.1878357756137845, 1.1741160671740725]\n",
      "\n",
      ") \n",
      "29400: \n",
      "\tD: 0.6745948195457458/0.4914208948612213 \n",
      "\tG: 0.8729363083839417 (Real: [4.1580439494550232, 1.3197404997668889], Fake: [3.7776801592111586, 1.4196401745525726]\n",
      "\n",
      ") \n",
      "29600: \n",
      "\tD: 0.6289479732513428/0.7492201924324036 \n",
      "\tG: 0.6999359726905823 (Real: [4.1730229532718655, 1.2165439456104497], Fake: [4.0162911754846569, 1.1883614197451096]\n",
      "\n",
      ") \n",
      "29800: \n",
      "\tD: 0.6472647786140442/0.7794463038444519 \n",
      "\tG: 0.6819497346878052 (Real: [3.9838246905803683, 1.2208820774233686], Fake: [4.0376683598756786, 1.2144939924554736]\n",
      "\n",
      ") \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for d_index in range(d_steps):  # train how many times per epoch\n",
    "        # 1. Train D on real + fake\n",
    "        D.zero_grad()\n",
    "        \n",
    "        # 1.1 Train D on real\n",
    "        d_real_data = Variable(d_sampler(d_input_size))  # every time, create a new set of normal dist. data\n",
    "                                                        # with numpy [1,100] input size\n",
    "        d_real_decision = D(preprocess(d_real_data))  # Preprocess to add the variance in the next 100 values as [1,100]\n",
    "                                                        # means, now the input numpy array is [1,200]\n",
    "                                                        # then put into the D: Discriminator to \n",
    "        d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones is real, and all the real samples are real\n",
    "        d_real_error.backward()  # BP in Discriminator\n",
    "        \n",
    "        # 1.2 Train D on fake\n",
    "        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))  # every time, create a new set of uniform dist. data.\n",
    "                                                                        # with numpy [100, 1] input size\n",
    "        d_fake_data = G(d_gen_input).detach() # IMPORTANT: detach the G model in this Discriminator's training\n",
    "                                                # to avoid training G on these labels\n",
    "        d_fake_decision = D(preprocess(d_fake_data.t()))  # 1st: transpose the output [100, 1] to [1, 100]\n",
    "                                                            #preprocess to add the variance as before\n",
    "                                                            # and put into the D\n",
    "        d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = fake\n",
    "        d_fake_error.backward()  # BP 2nd time in Discriminator\n",
    "        d_optimizer.step()   # Only optimize D parameters\n",
    "    \n",
    "    for g_index in range(g_steps):\n",
    "        # 2. Train G on D's reponse (but DO NOT train D on these labels)\n",
    "        G.zero_grad()\n",
    "        \n",
    "        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "        g_fake_data = G(gen_input)\n",
    "        dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "        g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))\n",
    "        \n",
    "        g_error.backward()\n",
    "        g_optimizer.step()  # Only optimize G's parameters\n",
    "        \n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"%s: \\n\\tD: %s/%s \\n\\tG: %s (Real: %s, Fake: %s\\n\\n) \" % (epoch,\n",
    "                                                           extract(d_real_error)[0],\n",
    "                                                           extract(d_fake_error)[0],\n",
    "                                                           extract(g_error)[0],\n",
    "                                                           stats(extract(d_real_data)),\n",
    "                                                           stats(extract(d_fake_data))))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.0976\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_fake_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 2.6421  4.9762  4.9099  3.6605  4.9133  3.2068  3.2556  1.2128  4.6405  4.8750\n",
       "\n",
       "Columns 10 to 19 \n",
       " 4.9206  1.4679  4.9114  2.2452  4.9841  4.8027  4.0996  4.6295  1.0627  4.9317\n",
       "\n",
       "Columns 20 to 29 \n",
       " 4.9654  4.5541  4.9681  2.0035  4.6235  4.9059  4.9117  4.5387  4.9293  4.3482\n",
       "\n",
       "Columns 30 to 39 \n",
       " 3.7267  4.9882  4.6924  2.0403  2.7674  4.9556  4.8169  1.3976  2.4600  4.8549\n",
       "\n",
       "Columns 40 to 49 \n",
       " 4.6632  1.0923  3.1289  4.9273  4.6444  4.7745  4.5708  4.8405  2.0023  1.9889\n",
       "\n",
       "Columns 50 to 59 \n",
       " 4.9691  4.2033  4.9812  0.9254  4.4897  2.9293  4.9708  2.9877  4.9508  4.3151\n",
       "\n",
       "Columns 60 to 69 \n",
       " 3.1709  1.3105  4.9690  4.3087  4.8409  4.7681  4.9797  4.8805  4.9752  4.2738\n",
       "\n",
       "Columns 70 to 79 \n",
       " 4.8113  4.5681  4.9379  4.8889  4.9809  4.9235  2.1977  4.4270  4.9750  3.0322\n",
       "\n",
       "Columns 80 to 89 \n",
       " 4.9765  4.3290  4.7503  4.2531  4.0633  4.9627  4.7380  4.7271  4.9697  1.0641\n",
       "\n",
       "Columns 90 to 99 \n",
       " 4.6159  4.9092  4.9783  4.7074  4.8971  4.8670  4.1339  3.5248  4.9533  4.9660\n",
       "[torch.FloatTensor of size 1x100]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_fake_data.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dig Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.15339613  4.16190147  2.62423015  2.98374081  4.79948854  5.29628515\n",
      "   4.62047291  3.76114655  3.13267255  3.81932688  2.78448296  4.24478626\n",
      "   4.74561977  4.44313145  4.64703178  3.67684364  4.40762281  4.10781097\n",
      "   3.96675086  4.97107792  3.43234563  5.46415281  5.09823275  5.26046371\n",
      "   3.5444746   3.77776909  4.29387283  1.86991048  2.51971817  4.17556763\n",
      "   4.87572098  5.15760088  4.65751505  3.42170572  4.13643837  1.95163918\n",
      "   2.73850989  2.75134802  3.78412795  3.72884369  5.75201225  2.97849011\n",
      "   4.49212933  5.43780279  6.62907171  2.12019253  3.31878114  2.71677542\n",
      "   4.2525773   7.74324989  4.98329115  7.01721144  2.70801234  4.55108452\n",
      "   4.1239934   6.97865534  3.61467934  2.47488236  4.66306639  4.77106428\n",
      "   1.92913306  2.38413596  3.95918131  5.21180201  3.40874147  2.92393208\n",
      "   4.14932871  4.93587446  2.68554997  5.81263542  4.75530481  6.25189114\n",
      "   2.91022396  5.12980032  2.93874741  6.62317944  5.27265835  4.33529091\n",
      "   5.14923477  5.02518177  6.36235809  5.32371998  5.71750307  4.84097958\n",
      "   2.39754081  2.19287252  4.93040466  3.04197311  4.99970245  2.70518708\n",
      "   3.25048685  4.12488508  3.95715904  2.75379252  3.66985989  3.83335876\n",
      "   1.88920426  1.71924901  3.91910911  4.00742483  3.33783412  1.63558984\n",
      "   2.67849946  4.73174572  3.40471268  5.33813858  5.00054312  1.88185084\n",
      "   4.17262459  4.69995403  5.84013414  3.76866889  4.45814371  5.88777161\n",
      "   3.33886051  5.60494232  5.0388031   3.21429062  2.76713538  4.24083996\n",
      "   3.31856108  4.92257166  6.6894269   5.18135738  2.93041229  6.27323151\n",
      "   2.67665195  3.37296081  3.49626517  2.94085002  3.48364401  6.11835527\n",
      "   4.43714905  5.67314005  2.54735446  2.70675945  3.7767005   4.89410353\n",
      "   3.93993568  2.46212602  3.87092638  4.48888206  4.9953599   4.30348158\n",
      "   3.8285749   3.10967374  4.92345142  3.86265922  4.61806822  3.67364407\n",
      "   4.720819    4.4892807   2.4346714   4.67582941  1.14680767  3.0898726\n",
      "   5.38223886  2.88963008  3.20687795  3.55361485  4.51227283  4.43680811\n",
      "   4.64081478  3.25018024  3.73439527  4.18450022  3.00584579  5.21897984\n",
      "   5.66592503  5.6035676   3.59552288  5.12321663  4.74445963  3.45000982\n",
      "   4.48617125  3.12086463  4.24141407  4.60455608  4.70905447  4.44858789\n",
      "   3.57696986  4.65423965  5.14624023  4.70975399  5.78619432  2.64042735\n",
      "   2.59641123  3.41224957  2.95710015  3.15310597  4.07723188  2.9129138\n",
      "   5.52615309  5.15104818  5.17956781  5.52002859  5.38773775  3.56772065\n",
      "   4.23290873  4.53526402  2.99830818  6.75131273  2.12394905  3.89877009\n",
      "   4.34955025  2.5148406   3.9899559   3.63322783  4.90673113  3.65332389\n",
      "   3.68852997  1.84436274  2.74590278  2.40565729  2.274544    3.36943388\n",
      "   3.59003329  2.07405162  4.30181551  3.03611112  3.73520756  1.56864631\n",
      "   2.67729855  6.00248194  4.35478926  2.45307136  2.490031    4.19677591\n",
      "   3.90859509  3.8570869   5.57350063  7.4782691   1.89170241  3.76726651\n",
      "   2.32091689  4.41158676  6.5635581   3.74670005  1.54819846  2.21242905\n",
      "   5.13573456  2.71008277  4.01134539  2.44351673  3.59147573  2.43774772\n",
      "   4.2598381   2.24823332  3.20014834  3.34539557  6.06271696  3.35103679\n",
      "   4.70120955  4.97994518  5.7394743   4.91103506  3.4862442   3.74904561\n",
      "   4.30784702  4.67914391  4.41039896  5.11910391  6.98095465  4.97558975\n",
      "   3.12454152  6.4688468   4.8459301   3.95014977  3.60250878  6.10014534\n",
      "   4.97890902  2.93835783  4.31095171  4.91477919  2.073771    3.07616186\n",
      "   3.4148159   5.24048519  3.74710894  7.06306362  1.59126973  2.4761374\n",
      "   3.35294628  3.92087078  4.50001335  1.70834005  1.98158407  3.3498497\n",
      "   3.32311797  4.27918816  5.20598698  2.5261724   3.74720049  3.89467216\n",
      "   4.0169425   4.8300004   2.65362382  3.6207931   3.72743678  3.88238406\n",
      "   3.36669469  5.42295361  3.35651112  6.60623074  2.34655285  4.77094126\n",
      "   5.25607777  4.35591936  3.36469889  2.26095557  2.59043121  4.48812294\n",
      "   5.16086626  3.80054426  3.48388886  5.08555079  5.20840788  4.02244616\n",
      "   2.97422409  6.08449459  6.29011059  2.79078794  3.67051554  5.27845335\n",
      "   3.03406239  3.86403108  3.02213502  5.04488277  2.61849594  2.72613215\n",
      "   3.41812754  2.36960959  4.12657022  1.84393728  6.14613771  3.7015605\n",
      "   2.72021198  3.07524323  1.85243368  5.45011282  4.83958578  5.87487602\n",
      "   7.46953535  4.06649446  3.79508424  5.09448433  4.2053051   4.06143713\n",
      "   2.09517717  3.53114438  5.58863449  2.60730505  3.58926773  2.98297501\n",
      "   4.87272787  4.95130301  2.54766917  3.98725772  4.01264048  6.09345579\n",
      "   5.12739515  4.17884636  5.65820694  2.7175014   3.47385526  2.28102398\n",
      "   3.28295851  3.64404607  3.10377598  4.15409184  3.72101092  3.15977383\n",
      "   5.91377831  3.76073074  4.15612984  3.21737719  4.64032173  3.35225201\n",
      "   4.09466839  3.36151481  5.6891346   3.77912927  6.16224003  3.38469076\n",
      "   4.5413661   3.04311967  3.67988133  4.59034729  5.78307486  4.22115231\n",
      "   5.57211924  5.97814226  3.88798642  1.30312908  4.1643734   1.39807391\n",
      "   3.49628019  3.71861911  3.48715591  3.62368441  4.88802385  2.37403703\n",
      "   2.64412093  5.58648729  3.56169939  4.1895442   2.70524955  3.38818216\n",
      "   3.79954457  4.82638121  3.75431752  4.4588542   4.50971889  3.5019474\n",
      "   4.64719725  4.14683104  6.14262342  3.3765192   4.93378592  2.66737056\n",
      "   6.02399254  4.98474789  4.75324535  4.38865519  2.82913351  4.93816185\n",
      "   4.06096077  3.66802216  6.99956512  3.56857586  2.29754758  3.34333825\n",
      "   4.04516268  3.08304238  3.5812974   2.82368445  5.58228111  5.11636066\n",
      "   3.65566373  2.65383101  2.39712143  2.99388337  3.66329885  2.96589875\n",
      "   3.52340174  7.16527414  3.31325603  3.20103073  3.20835161  3.0305686\n",
      "   3.98342967  3.29962516  1.15294921  1.77995014  4.48024845  4.77641487\n",
      "   4.05914402  3.76906991  5.33560801  2.48292041  3.60423875  4.8133707\n",
      "   4.90562248  5.77952719  3.03821182  5.21306086  3.26718593  6.63831949\n",
      "   5.09359884  3.59517336  4.67009306  3.42199659  2.83796549  2.53790307\n",
      "   3.91235018  4.94692373  3.08970976  3.02482939  3.18113112  3.67066407\n",
      "   3.34440231  5.28697443  3.1274159   3.6492672   3.3844173   3.68715024\n",
      "   4.0553627   2.14551187  4.09749317  4.32878542  2.59527802  3.44168901\n",
      "   3.93228674  1.23961294  4.17323065  4.87136555  5.27890205  3.31227207\n",
      "   6.83100939  4.27717209  2.24990726  2.38566208  5.10005188  5.06838179\n",
      "   6.32601547  4.42935801  1.21638393  2.49551511  1.83742654  4.73555136\n",
      "   3.67845535  3.6750071   4.96795559  6.60497141  3.47724414  3.28202248\n",
      "   4.89236832  4.88188791  3.96545053  2.08465147  3.03558683  3.50024748\n",
      "   3.29729342  2.57491016  5.65480232  4.59929466  3.45989227  3.86225772\n",
      "   5.16274548  2.74820638  5.25545931  4.77581739  3.79976845  1.86242449\n",
      "   2.91150808  3.30444098  5.55592108  3.30605316  6.00983143  3.88980222\n",
      "   3.06988049  5.74321079  3.92433953  4.88212013  3.65179563  4.4076972\n",
      "   3.23705339  4.18502665  1.85546207  4.40879154  5.13977861  2.27241731\n",
      "   2.46001339  5.3127799   3.66641045  5.28426361  3.44410133  3.29813552\n",
      "   2.41495991  2.51338363  4.8023138   4.36798382  2.6038208   2.81388855\n",
      "   5.01037788  3.46069813  4.62161064  3.42211747  4.43347216  5.25481224\n",
      "   2.81180573  4.11628103  4.65780783  5.97271013  5.241889    4.29256868\n",
      "   1.54954398  3.00159574  3.64135337  3.30520415  3.39715719  2.19994354\n",
      "   3.48204494  3.99246311  1.79971576  4.79626656  4.16506147  2.24183512\n",
      "   4.14060974  4.07792282  2.32836223  6.50845289  5.51954699  6.88308907\n",
      "   4.53696251  5.72421455  4.63269901  4.12821054  1.19201124  4.19229221\n",
      "   5.90817356  3.77180648  3.97897077  3.88551354  4.42043495  5.80439377\n",
      "   2.27820873  6.89622736  4.38164139  5.37750769  2.33706737  4.43432093\n",
      "   4.48292828  2.41322494  5.70675898  5.64789629  3.84312248  3.20313311\n",
      "   3.12483907  3.76444101  4.30929375  2.5976944   6.11892653  4.91982794\n",
      "   2.13035154  3.60274076  5.4947238   1.82834649  5.40752029  3.33898997\n",
      "   3.18049145  7.03896713  4.16848564  5.33383274  4.30968237  3.37795186\n",
      "   5.01703787  3.93227196  5.0696454   4.6226244   4.59712791  5.22799253\n",
      "   4.34594202  5.79638004  2.61713529  4.3309617   4.88982391  6.28751135\n",
      "   5.46618509  5.32519245  4.03344345  2.3845737   4.29941988  4.15964651\n",
      "   3.23905969  3.8774817   4.64876652  3.25490212  4.25346661  4.05148554\n",
      "   6.78947496  3.85769105  4.7841239   2.62383771  4.28199482  4.60398865\n",
      "   3.76581788  2.84256935  5.35229158  3.06419611  4.96045923  1.54892945\n",
      "   4.06040907  3.48416591  2.43912268  1.65190732  2.1954782   4.4656353\n",
      "   4.34239149  2.67826009  3.82769608  3.61241841  2.07344294  4.20772648\n",
      "   3.09847999  3.11429739  2.31953216  2.70112824  3.17990565  4.39051723\n",
      "   2.9945116   5.17989492  3.70033526  2.98019958  5.8758502   1.7527709\n",
      "   4.72634077  6.69832325  3.27927995  5.44304848  1.94023025  4.31249285\n",
      "   4.35783529  3.03553319  3.47766113  1.68910646  3.84656191  2.08252764\n",
      "   3.28427792  2.58279514  3.02515078  3.5399816   7.61973476  4.14271402\n",
      "   3.75094724  4.3990531   5.52302504  4.78270197  5.05486488  4.07017231\n",
      "   2.7827177   8.13922691  2.63695788  4.9417429   4.85690451  2.23242116\n",
      "   5.17231846  4.00063658  6.21283865  4.6372838   1.75003886  5.55774689\n",
      "   2.85567045  2.49668503  5.6452446   5.34745264  6.25233746  6.55587292\n",
      "   4.03959084  4.68068266  2.77438998  3.95326519  4.89518452  5.66115332\n",
      "   3.80546951  3.50950551  5.02702093  1.65316749  5.48659325  4.08522129\n",
      "   3.32828021  2.08997583  4.36062574  3.50445485  4.51792908  4.05137157\n",
      "   3.66013861  5.61322641  3.63946772  2.65870595  3.58956981  5.99857616\n",
      "   4.48220015  4.74074841  4.93676186  3.01596355  3.88688421  3.11842227\n",
      "   5.49306917  5.20505714  1.58266485  3.03989577  4.32660103  4.51898479\n",
      "   4.68255329  3.94108915  3.17329979  2.56938314  3.55775404  5.46806765\n",
      "   6.5377059   5.38661385  2.8970325   5.15102243  1.57679152  2.73390937\n",
      "   4.69323778  3.83440566  3.60430408  1.96887326  3.64983463  4.30037308\n",
      "   5.63834     3.97635031  3.77046442  3.81913137  3.03103805  2.1572876\n",
      "   4.86303186  1.41903746  6.50030184  3.31025696  5.54765034  3.66992712\n",
      "   4.13081121  4.68081951  6.87351179  2.25278401  5.86760807  3.11295795\n",
      "   5.80826521  5.36198473  2.7678318   4.46686649  3.00760341  4.54862404\n",
      "   6.40204287  3.13746262  3.09614468  5.75171518  3.16192627  3.45198154\n",
      "   4.48134613  4.60962677  3.70673561  4.40346909  4.62650824  6.09605646\n",
      "   4.49051619  3.07495284  2.41861987  3.33148289  5.16243172  2.54701447\n",
      "   2.74822736  1.47504961  0.67378837  2.98732495  2.69833636  3.02495003\n",
      "   5.89346361  3.29900384  3.93041229  5.85976982  2.1618638   1.70860863\n",
      "   4.92064142  4.23314953  4.57978821  3.9779346   4.78505516  3.76363206\n",
      "   5.78651762  3.97904277  5.33717966  7.15701151  5.83413887  4.24987507\n",
      "   4.83645105  3.24627781  5.82000256  1.07152534  5.29026365  2.03274775\n",
      "   4.63223743  3.63232803  3.24286795  5.15799665  3.7767911   2.07430196\n",
      "   3.24961758  5.03444242  2.44858336  3.95412612  3.61904383  5.23417377\n",
      "   4.18434334  4.86748791  5.53140926  4.5187335   2.45374107  2.32842159\n",
      "   4.77695131  3.50493908  6.15209818  6.17272806  3.77668118  3.29171348\n",
      "   3.6283927   3.63809395  1.07973826  4.04615688  4.35098028  3.05051661\n",
      "   4.46874762  3.80766249  2.86616445  2.67387843  3.09966755  2.7951467\n",
      "   3.06726384  4.6532774   2.11388206  5.16939974  3.31859422  4.16734791\n",
      "   4.11561537  4.1119113   4.2416296   5.66405964  4.68303442  3.47603273\n",
      "   2.72557735  2.81721568  5.80317974  4.23522186  3.97146368  4.47945118\n",
      "   2.41695499  4.3945899   3.6421566   3.81057811  3.907372    4.0396347\n",
      "   5.30076456  5.02112246  2.36775064  4.56672525  2.93278885  3.00900102\n",
      "   5.19303989  3.34570813  2.91295433  3.84974384  1.5586313   5.75745344\n",
      "   2.90329051  3.62008452  2.12580156  3.6575036   3.30154443  4.07871389\n",
      "   4.40421152  2.84575701  4.02883911  3.30581951  2.52489638  2.64486933\n",
      "   4.89228487  7.54375648  3.63098407  3.58278251  3.90744591  3.76565552\n",
      "   3.63884616  5.14992905  2.31557393  3.41953921  4.92534876  4.12455988\n",
      "   5.06515837  4.20870543  4.94008827  4.90024757  4.66196823  5.20793629\n",
      "   3.61989474  4.8761692   3.02352834  4.64835501  2.2256124   4.12085724\n",
      "   3.5572722   3.64721012  5.05554104  3.12990403  3.68716049  4.69619894\n",
      "   5.53658438  6.08063173  2.84785128  3.56355953  2.80579782  4.07400036\n",
      "   4.70292616  4.72681856  2.66724801  4.00428581  5.11085892  4.2992754\n",
      "   3.45147657  1.3245877   2.64167929  5.75730848  4.89787912  4.04406309\n",
      "   2.61907244  3.2499032   3.06297135  4.60141516]]\n",
      "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
      "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])]\n",
      "(1, 100)\n",
      "(1, 1000)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aadf780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_np = d_sampler(1000).numpy()\n",
    "\n",
    "print(sample_np)\n",
    "np.size(sample_np)\n",
    "\n",
    "a_np = sample_np.copy()\n",
    "a_x = [np.arange(100)]\n",
    "\n",
    "print(a_x)\n",
    "print(np.shape(a_x))\n",
    "print(np.shape(a_np))\n",
    "\n",
    "plt.hist(np.transpose(a_np)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29999: \n",
      "\tD: 0.5531200170516968/0.717462420463562 \n",
      "G: 0.7971842288970947 \n",
      "(Real: [4.1114702498912807, 1.1424093387516128], Fake: [3.9775783205032349, 1.2874585026157108]) \n"
     ]
    }
   ],
   "source": [
    "print(\"%s: \\n\\tD: %s/%s \\nG: %s \\n(Real: %s, Fake: %s) \" % (epoch,\n",
    "                                                           extract(d_real_error)[0],\n",
    "                                                           extract(d_fake_error)[0],\n",
    "                                                           extract(g_error)[0],\n",
    "                                                           stats(extract(d_real_data)),\n",
    "                                                           stats(extract(d_fake_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Comparison"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decorate_with_diffs(data, exponent):\n",
    "    mean = torch.mean(data.data, 1)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    return torch.cat([data, diffs], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean = torch.mean(d_real_data.data, 1)\n",
    "# print(d_real_data.data)\n",
    "# print(torch.mul(torch.ones(d_real_data.size()), mean.tolist()[0][0]))\n",
    "\n",
    "# print(torch.cat([d_real_data, torch.pow(d_real_data - Variable(torch.mul(torch.ones(d_real_data.size()), mean.tolist()[0][0])), 2.0)], 1))\n",
    "# print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(preprocess(d_real_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
