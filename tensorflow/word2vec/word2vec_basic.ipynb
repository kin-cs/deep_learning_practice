{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Basic word2vec example.\n",
    "From Tensorflow's Official Github\n",
    "Read this for the details: https://www.tensorflow.org/tutorials/word2vec\n",
    "\n",
    "As a practice and commented and modified by Kin\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified (correct size) text8.zip\n",
      "Data size (num of items in the list) 17005207\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the data\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    '''Download a file if not present'''\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified (correct size)', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "        'Failed to verify {}. Can you get to it by yourself?'.format(filename))\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "# Read the data into a list of strings\n",
    "def read_data(filename):\n",
    "    '''Extract the first file enclosed in a zip file as a list of words'''\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size (num of items in the list)', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking\n",
    "# vocabulary[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "\n",
    "vocabulary_size = 50000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    '''Process raw inputs into a dataset'''\n",
    "    count = [['UNK', -1]]\n",
    "    # get the most common 50000 words as the basic of the dictionary\n",
    "    # Counter() for count the \n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    # create the dict for the whole corpus, with numbers\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)  # [Good, but maybe slower] for putting numbers into dict\n",
    "    # create a data list by using the corpus's number-encoding\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count +=1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reversed_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)\n",
    "\n",
    "del vocabulary  # [Good Practice] to reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most commom words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "\n",
      "Sample Data [5235, 3083, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 Most commom words (+UNK)', count[:5])\n",
    "print('\\nSample Data', data[:10], [reversed_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 % len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Function to generate a training batch for the skip-gram model\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    # : Assert the correct arguments \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window | target | skip_window],\n",
    "                                # here the window size is 1, so span is 3\n",
    "    \n",
    "    # [Buffer (A Span)] create a Deque list with fixed size\n",
    "    buffer = collections.deque(maxlen=span)  # now span is 3\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "#     print(buffer)    \n",
    "    #  >____<\"\n",
    "    for i in range(batch_size // num_skips):  # : use // to get the integer for range\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "#         print('==i:', i,'- target:', target, 'target_avoid:', targets_to_avoid)\n",
    "        for j in range(num_skips):\n",
    "#             print('j:', j)\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)  # to randomly select the skip_window (0 or 2)\n",
    "#                 print('target in while loop', target)\n",
    "            targets_to_avoid.append(target)\n",
    "#             print('target_avoid:', targets_to_avoid)\n",
    "            batch[i * num_skips + j ] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "#             print('buffer[skip_window](batch):', i*2+j,' ', buffer[skip_window],'  buffer[skip_window]:',buffer[target])\n",
    "        buffer.append(data[data_index])\n",
    "#         print('\\n', buffer)\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3083 originated -> 12 as\n",
      "3083 originated -> 5235 anarchism\n",
      "12 as -> 3083 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(batch[i], reversed_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reversed_dictionary[labels[i, 0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "==== Step 0\n",
      "Nearest to would: dragster, prophylactic, interviewed, mez, kossuth, ataxia, corner, hooked,\n",
      "Nearest to into: uniquely, splendor, adultery, tack, woven, insulated, here, bayonets,\n",
      "Nearest to for: objectivity, occurred, ascending, dayan, miscellaneous, nymph, worshipped, lundy,\n",
      "Nearest to other: propelled, gpled, thereby, paglia, cadets, pj, impartial, unedited,\n",
      "Nearest to he: honduran, sitter, respects, muds, hertz, blackberry, melt, plucking,\n",
      "Nearest to world: choose, persists, ohlin, danner, convincingly, moisture, circular, kenyan,\n",
      "Nearest to of: the, hyperlinks, nawab, worldview, reducible, mur, rutgers, veda,\n",
      "Nearest to will: borealis, himalayas, culmination, infarction, reclaimed, gambler, foods, value,\n",
      "Nearest to all: amaranth, ft, lynyrd, lighters, undead, strictly, employ, inscription,\n",
      "Nearest to eight: blocking, adolphe, spotters, theocracy, conquests, identically, az, glucagon,\n",
      "Nearest to b: karachi, cues, leakey, blackboard, staunchly, retractable, ciphertexts, sidewalk,\n",
      "Nearest to there: examples, elicit, onlookers, chirality, laborers, poetical, hazel, meteorologist,\n",
      "Nearest to from: aromatic, tetrarchy, airforce, diabetic, shouted, diligent, jojo, orientations,\n",
      "Nearest to has: proving, bigcup, greenish, zap, prominence, dissolved, hooke, adjectives,\n",
      "Nearest to had: ecosystem, unconsciousness, deputy, fmln, convertible, commune, sol, hautes,\n",
      "Nearest to not: uncountably, ibizan, favoritism, wanderer, democrat, anabolic, digraph, jirga,\n",
      "\n",
      "==== Step 10000\n",
      "Nearest to would: could, without, against, after, world, when, should, him,\n",
      "Nearest to into: which, an, at, had, from, but, however, on,\n",
      "Nearest to for: on, from, this, as, were, with, at, by,\n",
      "Nearest to other: this, on, for, an, its, by, some, to,\n",
      "Nearest to he: his, from, that, was, which, this, but, had,\n",
      "Nearest to world: ii, would, other, an, with, being, without, united,\n",
      "Nearest to of: the, and, to, a, in, was, with, as,\n",
      "Nearest to will: for, much, if, this, can, on, so, many,\n",
      "Nearest to all: from, these, their, what, but, however, which, had,\n",
      "Nearest to eight: one, three, two, six, seven, nine, four, an,\n",
      "Nearest to b: french, from, american, over, with, so, this, english,\n",
      "Nearest to there: they, these, what, not, have, being, than, all,\n",
      "Nearest to from: on, with, for, were, this, which, at, although,\n",
      "Nearest to has: were, which, on, for, had, although, been, are,\n",
      "Nearest to had: which, but, as, when, he, into, however, from,\n",
      "Nearest to not: that, this, but, be, without, only, he, even,\n",
      "\n",
      "==== Step 20000\n",
      "Nearest to would: can, is, could, that, may, will, but, however,\n",
      "Nearest to into: being, but, between, for, however, an, were, in,\n",
      "Nearest to for: of, but, in, by, that, or, at, however,\n",
      "Nearest to other: at, its, their, which, some, with, for, being,\n",
      "Nearest to he: which, who, it, this, that, now, his, but,\n",
      "Nearest to world: ii, under, being, first, american, this, with, between,\n",
      "Nearest to of: and, for, in, by, to, the, or, a,\n",
      "Nearest to will: can, only, would, that, may, being, could, were,\n",
      "Nearest to all: its, which, being, their, some, a, any, however,\n",
      "Nearest to eight: six, three, five, four, nine, one, two, seven,\n",
      "Nearest to b: d, american, british, french, english, german, great, after,\n",
      "Nearest to there: they, which, now, it, who, often, not, no,\n",
      "Nearest to from: in, by, as, is, however, with, of, which,\n",
      "Nearest to has: is, had, can, however, only, were, when, are,\n",
      "Nearest to had: was, have, is, were, has, but, however, see,\n",
      "Nearest to not: it, that, they, but, which, only, he, have,\n",
      "\n",
      "==== Step 30000\n",
      "Nearest to would: may, can, could, to, will, but, however, only,\n",
      "Nearest to into: under, from, for, while, between, including, through, at,\n",
      "Nearest to for: on, under, while, from, however, at, when, with,\n",
      "Nearest to other: or, like, these, more, while, some, all, many,\n",
      "Nearest to he: then, who, it, usually, but, they, only, which,\n",
      "Nearest to world: first, under, however, later, in, with, a, non,\n",
      "Nearest to of: and, for, in, on, from, including, by, under,\n",
      "Nearest to will: can, could, may, would, only, however, to, then,\n",
      "Nearest to all: their, some, both, any, its, these, several, being,\n",
      "Nearest to eight: seven, six, five, four, nine, three, zero, one,\n",
      "Nearest to b: d, english, german, french, american, british, at, under,\n",
      "Nearest to there: it, they, no, still, not, which, he, who,\n",
      "Nearest to from: for, during, by, under, in, when, while, after,\n",
      "Nearest to has: had, have, if, but, however, was, although, were,\n",
      "Nearest to had: has, but, was, have, were, although, while, however,\n",
      "Nearest to not: still, often, they, who, then, there, that, also,\n",
      "\n",
      "==== Step 40000\n",
      "Nearest to would: could, will, can, may, must, if, was, only,\n",
      "Nearest to into: when, however, where, under, including, by, with, while,\n",
      "Nearest to for: including, under, with, by, against, and, without, within,\n",
      "Nearest to other: many, including, different, some, include, these, various, all,\n",
      "Nearest to he: still, who, they, it, which, she, that, then,\n",
      "Nearest to world: state, city, most, government, history, including, international, against,\n",
      "Nearest to of: in, and, during, by, for, under, like, while,\n",
      "Nearest to will: would, could, may, can, if, must, where, then,\n",
      "Nearest to all: some, many, any, because, both, these, its, several,\n",
      "Nearest to eight: four, seven, five, three, six, one, two, nine,\n",
      "Nearest to b: four, r, d, seven, and, three, eight, by,\n",
      "Nearest to there: which, they, still, usually, often, it, now, who,\n",
      "Nearest to from: during, under, by, and, using, or, in, among,\n",
      "Nearest to has: have, had, but, although, was, if, were, however,\n",
      "Nearest to had: have, has, were, although, was, if, then, when,\n",
      "Nearest to not: they, still, which, generally, often, he, usually, who,\n",
      "\n",
      "==== Step 50000\n",
      "Nearest to would: will, may, could, can, must, should, but, might,\n",
      "Nearest to into: from, through, with, within, including, by, over, between,\n",
      "Nearest to for: while, although, including, like, on, through, in, however,\n",
      "Nearest to other: several, many, these, including, modern, various, its, while,\n",
      "Nearest to he: it, she, they, who, still, there, no, often,\n",
      "Nearest to world: during, against, history, second, new, following, game, non,\n",
      "Nearest to of: from, near, without, for, in, including, during, within,\n",
      "Nearest to will: can, may, would, could, must, might, should, but,\n",
      "Nearest to all: some, both, each, another, these, any, because, which,\n",
      "Nearest to eight: three, seven, six, four, five, two, one, nine,\n",
      "Nearest to b: c, r, d, p, m, h, n, g,\n",
      "Nearest to there: still, now, usually, often, it, they, which, he,\n",
      "Nearest to from: including, within, by, during, for, or, into, on,\n",
      "Nearest to has: was, had, although, were, since, but, is, by,\n",
      "Nearest to had: were, has, was, but, although, have, when, since,\n",
      "Nearest to not: usually, still, who, they, also, often, generally, now,\n",
      "\n",
      "==== Step 60000\n",
      "Nearest to would: may, should, could, can, will, must, might, but,\n",
      "Nearest to into: within, through, by, from, between, under, during, without,\n",
      "Nearest to for: without, while, under, like, with, among, by, however,\n",
      "Nearest to other: these, various, many, especially, different, some, several, including,\n",
      "Nearest to he: she, it, they, usually, who, however, which, still,\n",
      "Nearest to world: ii, history, game, local, at, near, state, following,\n",
      "Nearest to of: in, whose, within, under, including, for, between, current,\n",
      "Nearest to will: would, may, could, can, must, should, might, then,\n",
      "Nearest to all: these, any, especially, some, instead, this, several, each,\n",
      "Nearest to eight: seven, nine, six, four, five, one, three, zero,\n",
      "Nearest to b: r, c, d, n, h, v, m, j,\n",
      "Nearest to there: it, still, now, they, usually, generally, which, he,\n",
      "Nearest to from: including, under, within, during, through, before, between, among,\n",
      "Nearest to has: had, but, having, have, were, however, without, was,\n",
      "Nearest to had: has, have, but, were, if, having, although, however,\n",
      "Nearest to not: usually, still, generally, now, never, they, it, often,\n",
      "\n",
      "==== Step 70000\n",
      "Nearest to would: will, may, might, could, should, must, can, were,\n",
      "Nearest to into: within, through, under, from, around, by, using, over,\n",
      "Nearest to for: like, including, although, within, while, from, under, using,\n",
      "Nearest to other: various, many, some, especially, different, several, local, these,\n",
      "Nearest to he: she, it, never, who, often, they, usually, itself,\n",
      "Nearest to world: ii, today, city, first, government, word, largest, island,\n",
      "Nearest to of: whose, in, including, within, or, include, like, although,\n",
      "Nearest to will: may, could, must, would, should, can, might, were,\n",
      "Nearest to all: these, some, instead, only, because, both, today, them,\n",
      "Nearest to eight: six, nine, seven, three, five, four, zero, two,\n",
      "Nearest to b: r, p, n, f, j, m, k, g,\n",
      "Nearest to there: it, usually, they, still, often, which, she, generally,\n",
      "Nearest to from: within, under, through, at, against, during, using, by,\n",
      "Nearest to has: have, had, was, were, but, having, will, thus,\n",
      "Nearest to had: has, was, have, were, when, having, but, never,\n",
      "Nearest to not: usually, generally, never, often, still, now, it, they,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Step 80000\n",
      "Nearest to would: might, must, will, may, should, could, can, cannot,\n",
      "Nearest to into: under, from, through, without, within, with, and, against,\n",
      "Nearest to for: in, using, without, during, within, through, under, or,\n",
      "Nearest to other: various, many, different, including, especially, specific, some, whose,\n",
      "Nearest to he: she, they, never, it, who, thus, and, originally,\n",
      "Nearest to world: today, city, ii, near, book, against, largest, during,\n",
      "Nearest to of: whose, within, including, through, like, in, between, near,\n",
      "Nearest to will: would, could, must, may, should, cannot, might, can,\n",
      "Nearest to all: some, these, both, many, each, instead, especially, various,\n",
      "Nearest to eight: seven, six, nine, five, four, three, two, one,\n",
      "Nearest to b: r, g, c, m, n, d, l, f,\n",
      "Nearest to there: which, it, still, they, thus, usually, she, who,\n",
      "Nearest to from: through, during, including, in, under, against, and, by,\n",
      "Nearest to has: had, have, is, was, having, but, since, when,\n",
      "Nearest to had: have, has, were, was, when, having, while, thus,\n",
      "Nearest to not: still, never, usually, it, they, generally, typically, always,\n",
      "\n",
      "==== Step 90000\n",
      "Nearest to would: might, will, must, could, should, may, can, cannot,\n",
      "Nearest to into: within, without, through, from, while, under, by, in,\n",
      "Nearest to for: within, or, in, as, by, using, include, while,\n",
      "Nearest to other: various, some, different, several, many, specific, these, historical,\n",
      "Nearest to he: she, it, they, never, who, but, eventually, we,\n",
      "Nearest to world: history, city, largest, region, country, business, standard, space,\n",
      "Nearest to of: including, from, in, and, whose, among, against, within,\n",
      "Nearest to will: could, would, must, might, cannot, should, can, may,\n",
      "Nearest to all: some, these, instead, many, both, several, any, various,\n",
      "Nearest to eight: seven, six, nine, five, three, four, two, zero,\n",
      "Nearest to b: m, l, c, r, d, f, n, david,\n",
      "Nearest to there: it, often, typically, they, which, usually, still, who,\n",
      "Nearest to from: within, through, while, and, in, including, under, without,\n",
      "Nearest to has: had, have, having, is, was, but, since, while,\n",
      "Nearest to had: has, have, was, having, since, were, but, where,\n",
      "Nearest to not: never, still, generally, always, often, typically, it, usually,\n",
      "\n",
      "==== Step 100000\n",
      "Nearest to would: could, will, might, should, must, cannot, may, can,\n",
      "Nearest to into: through, without, off, from, in, using, under, within,\n",
      "Nearest to for: using, without, within, including, by, under, or, like,\n",
      "Nearest to other: many, various, these, some, different, several, especially, particularly,\n",
      "Nearest to he: she, it, never, they, who, eventually, there, that,\n",
      "Nearest to world: ii, company, word, largest, region, team, movement, standard,\n",
      "Nearest to of: including, whose, like, or, and, without, specific, original,\n",
      "Nearest to will: might, must, should, would, could, cannot, can, may,\n",
      "Nearest to all: instead, these, many, several, some, because, players, any,\n",
      "Nearest to eight: seven, nine, four, six, three, five, zero, two,\n",
      "Nearest to b: d, UNK, c, k, r, robert, g, in,\n",
      "Nearest to there: now, it, still, they, typically, generally, usually, often,\n",
      "Nearest to from: within, under, before, in, during, throughout, through, by,\n",
      "Nearest to has: had, have, since, is, without, having, are, but,\n",
      "Nearest to had: has, have, since, is, was, when, were, by,\n",
      "Nearest to not: generally, still, now, often, typically, actually, never, usually,\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build the tensorflow Graph to train the model\n",
    "# using \"Noise-contrastive estimation\" Loss\n",
    "# Details of Candidate sampling: https://www.tensorflow.org/api_guides/python/nn#Candidate_Sampling\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "\n",
    "# Construct the validation set, with some most frequent words\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64  # num of negative samples\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # [4.1 - Set Placeholders/Constant] input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # [4.2 - Set Device] Ops and variables pin to the CPU (with GPU implememtation)\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1, 1))\n",
    "        # nn.embedding_lookup: help get the embedding for train_inputs(which are some indexes)\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    # [4.3 - Set Variable] Construct the variables for the NCE loss/ embed-to-output matrix\n",
    "    nce_weights = tf.Variable(\n",
    "                    tf.truncated_normal([vocabulary_size, embedding_size]))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # [4.4 - Loss Function] Compute the avg NCE loss for the batch\n",
    "    # tf.nce_loss automatically draws a new sample of the negative label each\n",
    "    # time we evaluate the loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,  # Weights\n",
    "                      biases=nce_biases,  # Biases\n",
    "                      labels=train_labels,  # Y\n",
    "                      inputs=embed,  # X\n",
    "                      num_sampled=num_sampled,  \n",
    "                      num_classes=vocabulary_size))\n",
    "    \n",
    "    # [4.5 - Optimization] Construct the SGD optimizer using a learning rate (1.0)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    # >___<'' [4.6 - Cosine similarity] Cosine Similarity between minibatch examples and all embeddings\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "    valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    # make a variable initializer\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# Step 5: Trainning Begins!\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # initialize all variables before use it\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        # Perform one update step by evaluating the optimizer op\n",
    "        # (including it in the list of returned values for sess.run())\n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        # This step is expensive ( ~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            print('\\n==== Step', step)\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # top k nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings (2D)\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne_1000.png'):   \n",
    "    assert low_dim_embs.shape[0] >=len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(36, 36))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                    xy=(x, y),\n",
    "                    xytext=(5,2),\n",
    "                    textcoords='offset points',\n",
    "                    ha='right',\n",
    "                    va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_top_n_words =1000\n",
    "    # Fit argument into an embedded space and return that transformed output.\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_top_n_words, :])\n",
    "    labels = [reversed_dictionary[i] for i in range(plot_top_n_words)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    \n",
    "except ImportError:\n",
    "    print('Please install all the basic dependencies as a deep learner.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
